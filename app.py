# -*- coding: utf-8 -*-

# ==============================================================================
# Krok 0: Instalacja bibliotek
# ==============================================================================
# Je≈õli uruchamiasz to lokalnie, upewnij siƒô, ≈ºe masz te biblioteki zainstalowane:
# pip install streamlit requests trafilatura google-generativeai google-api-python-client scrapingbee
# Je≈õli uruchamiasz w Streamlit Cloud, dodaj je do pliku requirements.txt

# ==============================================================================
# Krok 1: Import bibliotek
# ==============================================================================
import streamlit as st
import requests
import re
from trafilatura import extract
import google.generativeai as genai
# googleapiclient.discovery nie bƒôdzie ju≈º potrzebne do pobierania SERP,
# ale zostawiam na wypadek innych potencjalnych zastosowa≈Ñ Google API
from googleapiclient.discovery import build


# ==============================================================================
# Krok 2: Konfiguracja strony Streamlit
# ==============================================================================
st.set_page_config(page_title="SEO Content Powerhouse", page_icon="üöÄ", layout="wide")
st.title("üöÄ SEO Content Powerhouse z AI")
st.markdown("Narzƒôdzie do tworzenia kompletnych strategii contentowych na podstawie analizy TOP 10 wynik√≥w Google.")


# ==============================================================================
# Krok 3: Obs≈Çuga Kluczy API ze Streamlit Secrets
# ==============================================================================
try:
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
    # SEARCH_API_KEY i SEARCH_ENGINE_ID nie bƒôdƒÖ ju≈º u≈ºywane do pobierania SERP,
    # ale mogƒÖ byƒá przydatne, je≈õli zdecydujesz siƒô na inne funkcje Google API.
    # Je≈õli sƒÖ niepotrzebne, mo≈ºna je usunƒÖƒá z secrets i z tego bloku.
    if "SEARCH_API_KEY" in st.secrets: # Opcjonalne ≈Çadowanie
        SEARCH_API_KEY = st.secrets["SEARCH_API_KEY"]
    if "SEARCH_ENGINE_ID" in st.secrets: # Opcjonalne ≈Çadowanie
        SEARCH_ENGINE_ID = st.secrets["SEARCH_ENGINE_ID"]
    SCRAPINGBEE_API_KEY = st.secrets["SCRAPINGBEE_API_KEY"]

    genai.configure(api_key=GEMINI_API_KEY)

except KeyError as e:
    missing_key = str(e).strip("'")
    if missing_key == "SCRAPINGBEE_API_KEY" or missing_key == "GEMINI_API_KEY":
        st.error(f"üõë B≈ÇƒÖd konfiguracji sekret√≥w! Nie znaleziono wymaganego sekretu: {missing_key}. Upewnij siƒô, ≈ºe skonfigurowa≈Çe≈õ przynajmniej GEMINI_API_KEY i SCRAPINGBEE_API_KEY w ustawieniach Streamlit.")
        st.stop()
    else:
        st.warning(f"Uwaga: Nie znaleziono opcjonalnego sekretu: {missing_key}. Je≈õli nie planujesz u≈ºywaƒá funkcji z nim zwiƒÖzanych, mo≈ºesz to zignorowaƒá.")
except Exception as e:
    st.error(f"üõë WystƒÖpi≈Ç nieoczekiwany b≈ÇƒÖd podczas ≈Çadowania kluczy: {e}")
    st.stop()


# ==============================================================================
# Krok 4: Funkcje Backendowe
# ==============================================================================

@st.cache_data # Cache'owanie wynik√≥w wyszukiwania
def get_top_10_google_results_with_scrapingbee(api_key_sb, query, num_results=10, country_code='pl', language_code='pl'):
    """Pobiera wyniki wyszukiwania Google u≈ºywajƒÖc API ScrapingBee."""
    params = {
        'api_key': api_key_sb,
        'search': query,
        'nb_results': str(num_results),
        'country_code': country_code,
        'language': language_code, # Dodano parametr jƒôzyka
        # Mo≈ºesz dodaƒá inne parametry zgodnie z dokumentacjƒÖ ScrapingBee, np. 'device': 'desktop'
    }
    try:
        response = requests.get('https://app.scrapingbee.com/api/v1/', params=params, timeout=60) # Zwiƒôkszony timeout
        response.raise_for_status() # Rzuci wyjƒÖtek dla b≈Çƒôd√≥w HTTP
        data = response.json()

        if 'organic_results' in data and data['organic_results']:
            results = []
            for item in data['organic_results']:
                # Sprawdzamy, czy klucze 'title' i 'link' istniejƒÖ
                title = item.get('title')
                link = item.get('link')
                if title and link:
                    results.append({'title': title, 'link': link})
                else:
                    st.warning(f"Pominiƒôto wynik z ScrapingBee z powodu braku tytu≈Çu lub linku: {item}")
            return results
        else:
            st.warning(f"ScrapingBee nie zwr√≥ci≈Ço 'organic_results' dla zapytania: {query}. Odpowied≈∫: {data.get('error', data)}")
            return []

    except requests.exceptions.Timeout:
        st.error(f"üõë Przekroczono czas oczekiwania na odpowied≈∫ od ScrapingBee dla zapytania: {query}")
        return None
    except requests.exceptions.RequestException as e:
        st.error(f"üõë B≈ÇƒÖd podczas komunikacji z API ScrapingBee: {e}")
        return None
    except Exception as e: # Og√≥lny wyjƒÖtek dla np. problem√≥w z JSON
        st.error(f"üõë Nieoczekiwany b≈ÇƒÖd podczas przetwarzania odpowiedzi z ScrapingBee: {e}")
        return None


@st.cache_data
def scrape_and_clean_content(url_to_scrape, scrapingbee_api_key):
    """Pobiera i czy≈õci tre≈õƒá ze strony u≈ºywajƒÖc ScrapingBee (bez zmian)."""
    try:
        response = requests.get(
            url='https://app.scrapingbee.com/api/v1/',
            params={'api_key': scrapingbee_api_key, 'url': url_to_scrape, 'premium_proxy': 'true', 'block_resources': 'false'},
            timeout=90
        )
        response.raise_for_status()
        extracted_text = extract(response.text, include_comments=False, include_tables=False, include_images=False)
        if not extracted_text:
             return None
        cleaned_text = re.sub(r'\s+', ' ', extracted_text).strip()
        return cleaned_text if len(cleaned_text) > 100 else None
    except requests.exceptions.RequestException as e:
        st.warning(f"‚ö†Ô∏è Nie uda≈Ço siƒô pobraƒá tre≈õci z {url_to_scrape} (ScrapingBee): {e}")
        return None
    except Exception as e:
        st.warning(f"‚ö†Ô∏è WystƒÖpi≈Ç nieoczekiwany b≈ÇƒÖd podczas przetwarzania tre≈õci z {url_to_scrape}: {e}")
        return None


@st.cache_data(show_spinner="AI analizuje tre≈õƒá...")
def analyze_content_with_gemini(all_content, keyword_phrase):
    """Analizuje zagregowanƒÖ tre≈õƒá i generuje raport z Gemini (bez zmian)."""
    if not all_content:
        return "Brak tre≈õci do analizy przez AI."
    prompt = f"""
Jeste≈õ ≈õwiatowej klasy analitykiem SEO i strategiem content marketingu. Twoim zadaniem jest przeanalizowanie dostarczonej tre≈õci z czo≈Çowych artyku≈Ç√≥w dla frazy "{keyword_phrase}" i na tej podstawie wygenerowanie kompleksowego raportu w formacie Markdown.

Raport musi byƒá podzielony na DOK≈ÅADNIE nastƒôpujƒÖce sekcje, u≈ºywajƒÖc nag≈Ç√≥wk√≥w `### numer. Nazwa sekcji` i **≈ºadnych innych nag≈Ç√≥wk√≥w H3 w tytu≈Çach sekcji raportu**:

### 1. Kluczowe Punkty Wsp√≥lne
(Wypunktuj tematy, podtematy, kluczowe informacje, perspektywy i style narracji, kt√≥re powtarzajƒÖ siƒô w wiƒôkszo≈õci analizowanych tekst√≥w. Skup siƒô na tym, co jest standardem w TOP 10 i skonstruuj wytyczne dla copywritera)

### 2. Unikalne i Wyr√≥≈ºniajƒÖce Siƒô Elementy
(Wypunktuj nietypowe, oryginalne, innowacyjne lub szczeg√≥lnie warto≈õciowe informacje, dane, przyk≈Çady, case studies, infografiki (opisz co przedstawiajƒÖ) lub perspektywy, kt√≥re pojawi≈Çy siƒô tylko w niekt√≥rych ≈∫r√≥d≈Çach i mogƒÖ stanowiƒá przewagƒô konkurencyjnƒÖ dla nowego artyku≈Çu.)

### 3. Sugerowane S≈Çowa Kluczowe i Semantyka
(Na podstawie analizy tre≈õci konkurencji, stw√≥rz listƒô 10-12 najwa≈ºniejszych s≈Ç√≥w kluczowych, fraz d≈Çugoogonowych i pojƒôƒá semantycznie powiƒÖzanych. Pogrupuj je tematycznie, je≈õli to u≈Çatwia zrozumienie. Wska≈º intencjƒô wyszukiwania dla frazy g≈Ç√≥wnej.)

### 4. Proponowana Struktura Artyku≈Çu (Szkic)
(Zaproponuj idealnƒÖ, rozbudowanƒÖ strukturƒô nowego artyku≈Çu w formacie Markdown. U≈ºyj nag≈Ç√≥wk√≥w drugiego poziomu (`##`) dla g≈Ç√≥wnych sekcji i nag≈Ç√≥wk√≥w trzeciego poziomu (`###`) dla podpunkt√≥w. Zaproponuj kilka nag≈Ç√≥wk√≥w do artyku≈Çu, zawierajƒÖcych **oko≈Ço 3 nag≈Ç√≥wki H2 i 1 nag≈Ç√≥wek H3 jako przyk≈Çad hierarchii**. Uwzglƒôdnij kluczowe punkty, unikalne elementy i semantykƒô z analizy.)

### 5. Sekcja FAQ (Pytania i Odpowiedzi)
(Stw√≥rz listƒô 4-5 najczƒôstszych pyta≈Ñ, na kt√≥re odpowiadajƒÖ konkurenci, w stylu 'People Also Ask'. Podaj 2-3 zdaniowe bezpo≈õrednie odpowiedzi na te pytania, bazujƒÖc na analizowanej tre≈õci. Odpowiedzi napisz pod pytaniami)


Pamiƒôtaj, aby Twoja odpowied≈∫ by≈Ça TYLKO tre≈õciƒÖ raportu w formacie Markdown, bez ≈ºadnych dodatkowych wstƒôp√≥w czy podsumowa≈Ñ poza strukturƒÖ raportu. Ca≈Ça odpowied≈∫ musi byƒá w jƒôzyku polskim.
Tre≈õƒá do analizy:
{all_content}
"""
    try:
        model = genai.GenerativeModel('gemini-1.5-flash-latest')
        response = model.generate_content(prompt)
        if hasattr(response, 'text') and response.text:
             return response.text
        else:
             st.warning("‚ö†Ô∏è Gemini zwr√≥ci≈Ço pustƒÖ odpowied≈∫ lub b≈ÇƒÖd. Spr√≥buj ponownie lub zmie≈Ñ prompt.")
             if hasattr(response, 'prompt_feedback'): st.write("Feedback z promptu:", response.prompt_feedback)
             if hasattr(response, 'candidates') and response.candidates:
                  if response.candidates[0].finish_reason: st.write("Przyczyna zako≈Ñczenia:", response.candidates[0].finish_reason)
                  if hasattr(response.candidates[0], 'safety_ratings'): st.write("Oceny bezpiecze≈Ñstwa:", response.candidates[0].safety_ratings)
             return None
    except Exception as e:
        st.error(f"üõë B≈ÇƒÖd podczas komunikacji z Gemini API: {e}")
        return None

def parse_report(report_text):
    """Dzieli pe≈Çny raport na sekcje do wy≈õwietlenia w zak≈Çadkach (bez zmian)."""
    if not report_text: return {}
    sections = {}
    pattern = r"###\s*(?:\d+\.\s*)?(.*?)\n(.*?)(?=\n###\s*|$|\Z)"
    matches = re.findall(pattern, report_text, re.DOTALL)
    for match in matches:
        title = match[0].strip()
        content = match[1].strip()
        if title: sections[title] = content
    return sections

# ==============================================================================
# Krok 5: Interfejs U≈ºytkownika i g≈Ç√≥wna logika
# ==============================================================================

keyword = st.text_input("Wprowad≈∫ frazƒô kluczowƒÖ, kt√≥rƒÖ chcesz przeanalizowaƒá:", placeholder="np. jak dbaƒá o buty sk√≥rzane")

if st.button("üöÄ Wygeneruj Kompleksowy Audyt SEO"):
    if not keyword:
        st.warning("Proszƒô wpisaƒá frazƒô kluczowƒÖ.")
        st.stop()

    if 'SCRAPINGBEE_API_KEY' not in st.secrets or 'GEMINI_API_KEY' not in st.secrets:
         st.error("B≈ÇƒÖd: Klucze SCRAPINGBEE_API_KEY lub GEMINI_API_KEY nie sƒÖ skonfigurowane w Streamlit Secrets.")
         st.stop()

    with st.spinner("Przeprowadzam pe≈Çny audyt... To mo≈ºe potrwaƒá kilka minut."):
        st.info("Etap 1/4: Pobieranie i filtrowanie wynik√≥w z Google (przez ScrapingBee)...")
        
        # U≈ºywamy nowej funkcji z kluczem ScrapingBee
        top_results = get_top_10_google_results_with_scrapingbee(SCRAPINGBEE_API_KEY, keyword)

        if top_results is None: # Obs≈Çuga b≈Çƒôdu krytycznego z API
            st.error("WystƒÖpi≈Ç krytyczny b≈ÇƒÖd podczas pobierania wynik√≥w z ScrapingBee. Audyt przerwany.")
            st.stop()
        if not top_results:
            st.error(f"Nie znaleziono ≈ºadnych wynik√≥w TOP 10 dla frazy: '{keyword}' przy u≈ºyciu ScrapingBee. Spr√≥buj innej frazy.")
            st.stop()

        BANNED_DOMAINS = [
            "youtube.com", "pinterest.", "instagram.com", "facebook.com",
            "olx.pl", "allegro.pl", "twitter.com", "tiktok.com",
            "wikipedia.org", "s≈Çownik.pl", "encyklopedia.", "forum.",
            ".gov", ".edu", "otodom.pl", "gratka.pl", "domiporta.pl"
        ]
        filtered_results = [r for r in top_results if r and r.get('link') and not any(b in r['link'].lower() for b in BANNED_DOMAINS)]

        if not filtered_results:
            st.error("Po filtracji nie pozosta≈Çy ≈ºadne artyku≈Çy do analizy.")
            st.stop()

        if len(top_results) > len(filtered_results):
             st.info(f"Pominiƒôto {len(top_results) - len(filtered_results)} wynik√≥w, analizujƒô {len(filtered_results)} znalezionych artyku≈Ç√≥w.")

        st.subheader("Analizowane adresy URL (po filtracji):")
        for i, result in enumerate(filtered_results, 1):
            display_title = result.get('title', result.get('link', f"Brak tytu≈Çu dla {result.get('link', 'nieznany URL')}"))
            st.write(f"{i}. [{display_title}]({result.get('link', '#')})")

        st.info("Etap 2/4: Pobieranie tre≈õci ze stron przez Scraping API...")
        all_articles_content, successful_sources = [], []
        progress_bar = st.progress(0)
        for i, result in enumerate(filtered_results):
             url = result.get('link')
             if url:
                 content = scrape_and_clean_content(url, SCRAPINGBEE_API_KEY)
                 if content:
                     all_articles_content.append(content)
                     successful_sources.append({'title': result.get('title', 'Brak tytu≈Çu'), 'link': url})
                 progress_bar.progress((i + 1) / len(filtered_results))
        progress_bar.empty()

        if not all_articles_content:
            st.error("Nie uda≈Ço siƒô pobraƒá tre≈õci z ≈ºadnej ze stron. Sprawd≈∫ limity ScrapingBee lub dostƒôpno≈õƒá stron.")
            st.stop()
        st.success(f"‚úÖ Pomy≈õlnie pobrano tre≈õci z {len(all_articles_content)} stron.")

        st.info("Etap 3/4: Generowanie kompleksowego raportu przez AI...")
        aggregated_content = "\n\n---\n\n".join(all_articles_content)
        full_report = analyze_content_with_gemini(aggregated_content, keyword)

        if not full_report:
             st.error("Generowanie raportu przez Gemini nie powiod≈Ço siƒô.")
             st.stop()

        st.info("Etap 4/4: Formatowanie wynik√≥w...")
        report_sections = parse_report(full_report)
        sources_text = "\n".join([f"- [{source['title']}]({source['link']})" for source in successful_sources])
        report_sections["Analizowane ≈πr√≥d≈Ça"] = "Poni≈ºej lista adres√≥w URL, kt√≥rych tre≈õƒá zosta≈Ça pomy≈õlnie pobrana i przeanalizowana przez AI:\n" + sources_text

        st.balloons()
        st.success("‚úÖ Audyt SEO gotowy!")
        st.markdown(f"--- \n## Audyt SEO i plan tre≈õci dla frazy: '{keyword}'")

        preferred_tab_order = [
            "Kluczowe Punkty Wsp√≥lne", "Unikalne i Wyr√≥≈ºniajƒÖce Siƒô Elementy",
            "Sugerowane S≈Çowa Kluczowe i Semantyka", "Proponowana Struktura Artyku≈Çu (Szkic)",
            "Sekcja FAQ (Pytania i Odpowiedzi)", "Wnioski i Rekomendacje", "Analizowane ≈πr√≥d≈Ça"
        ]
        actual_tab_titles = [title for title in preferred_tab_order if title in report_sections and report_sections[title].strip()]
        if actual_tab_titles:
             sources_tab_title = "Analizowane ≈πr√≥d≈Ça"
             if sources_tab_title in actual_tab_titles: actual_tab_titles.remove(sources_tab_title)
             tabs_to_create = actual_tab_titles
             if sources_tab_title in report_sections and report_sections[sources_tab_title].strip():
                 tabs_to_create = actual_tab_titles + [sources_tab_title]
             if tabs_to_create:
                tabs = st.tabs(tabs_to_create)
                tab_title_map = {i: title for i, title in enumerate(tabs_to_create)}
                for i in range(len(tabs)):
                    with tabs[i]:
                        current_title = tab_title_map[i]
                        st.header(current_title)
                        st.markdown(report_sections[current_title])
             else: st.warning("Brak danych do wy≈õwietlenia w zak≈Çadkach po przetworzeniu.")
        else: st.warning("Brak danych do wy≈õwietlenia w zak≈Çadkach.")
else:
    if keyword: st.info(f"Wprowadzono frazƒô: '{keyword}'. Kliknij przycisk powy≈ºej, aby rozpoczƒÖƒá analizƒô.")
