# -*- coding: utf-8 -*-

# ==============================================================================
# Krok 0: Instalacja bibliotek
# ==============================================================================
# pip install streamlit requests trafilatura google-generativeai scrapingbee
# ==============================================================================
# Krok 1: Import bibliotek
# ==============================================================================
import streamlit as st
import requests
import re
from trafilatura import extract
import google.generativeai as genai
from urllib.parse import urlencode as encode_query_params
import json
import time

# ==============================================================================
# Krok 2: Konfiguracja strony Streamlit
# ==============================================================================
st.set_page_config(page_title="SEO Content Powerhouse", page_icon="üöÄ", layout="wide")
st.title("üöÄ SEO Content Powerhouse z AI")
st.markdown("Narzƒôdzie do tworzenia kompletnych strategii contentowych na podstawie analizy TOP 10 wynik√≥w Google.")

# ==============================================================================
# Krok 3: Obs≈Çuga Kluczy API ze Streamlit Secrets
# ==============================================================================
try:
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
    SCRAPINGBEE_API_KEY = st.secrets["SCRAPINGBEE_API_KEY"]
    DATAFORSEO_LOGIN = st.secrets["DATAFORSEO_LOGIN"]
    DATAFORSEO_PASSWORD = st.secrets["DATAFORSEO_PASSWORD"]

    genai.configure(api_key=GEMINI_API_KEY)

except KeyError as e:
    missing_key = str(e).strip("'")
    st.error(f"üõë B≈ÇƒÖd konfiguracji sekret√≥w! Nie znaleziono wymaganego sekretu: {missing_key}. Upewnij siƒô, ≈ºe skonfigurowa≈Çe≈õ GEMINI_API_KEY, SCRAPINGBEE_API_KEY, DATAFORSEO_LOGIN i DATAFORSEO_PASSWORD.")
    st.stop()
except Exception as e:
    st.error(f"üõë WystƒÖpi≈Ç nieoczekiwany b≈ÇƒÖd podczas ≈Çadowania kluczy: {e}")
    st.stop()

# ==============================================================================
# Krok 4: Funkcje Backendowe
# ==============================================================================

@st.cache_data
def get_serp_data_with_dataforseo(login, password, query, num_results=10, location_code=2616, language_code='pl'):
    """Pobiera TYLKO wyniki organiczne wyszukiwania Google u≈ºywajƒÖc API DataForSEO."""
    # (bez zmian)
    post_data = [{"keyword": query, "location_code": location_code, "language_code": language_code, "depth": num_results}]
    headers = {'Content-Type': 'application/json'}
    endpoint_url = "https://api.dataforseo.com/v3/serp/google/organic/live/regular"
    organic_results_list = []
    try:
        response = requests.post(endpoint_url, auth=(login, password), headers=headers, json=post_data, timeout=60)
        response.raise_for_status()
        data = response.json()
        if data.get("status_code") == 20000 and data.get("tasks") and data["tasks"][0].get("result") and data["tasks"][0]["result"][0].get("items"):
            items = data["tasks"][0]["result"][0]["items"]
            for item in items:
                if item.get("type") == "organic":
                    title, link = item.get("title"), item.get("url")
                    if title and link: organic_results_list.append({'title': title, 'link': link})
        else:
            status_message = data.get("status_message", "Nieznany b≈ÇƒÖd.")
            tasks_error = ""
            if data.get("tasks") and data["tasks"][0].get("status_message") != "Ok.":
                tasks_error = f" B≈ÇƒÖd zadania: {data['tasks'][0]['status_code']} - {data['tasks'][0]['status_message']}"
            st.warning(f"DataForSEO API (SERP) zwr√≥ci≈Ço nieoczekiwany status lub brak wynik√≥w: {status_message}{tasks_error}.")
        return organic_results_list
    except Exception as e: # Uproszczona obs≈Çuga b≈Çƒôd√≥w dla zwiƒôz≈Ço≈õci
        st.error(f"üõë B≈ÇƒÖd DataForSEO (SERP): {e}")
        return []


@st.cache_data
def get_keyword_volumes_dataforseo(login, password, keywords_list, location_code=2616, language_code='pl'):
    """Pobiera wolumeny wyszukiwa≈Ñ dla listy s≈Ç√≥w kluczowych z DataForSEO."""
    if not keywords_list:
        return {}

    # DataForSEO API pozwala na wys≈Çanie do 1000 s≈Ç√≥w kluczowych w jednym zadaniu,
    # ale w ramach jednego elementu tablicy 'post_data' do 100.
    # Dla bezpiecze≈Ñstwa i prostoty, je≈õli mamy wiƒôcej, mo≈ºna by to dzieliƒá na paczki,
    # ale na razie zak≈Çadamy, ≈ºe lista nie bƒôdzie a≈º tak d≈Çuga.
    post_data = [{
        "keywords": keywords_list,
        "location_code": location_code,
        "language_code": language_code
    }]
    
    headers = {'Content-Type': 'application/json'}
    # Endpoint dla Google Ads Search Volume API
    endpoint_url = "https://api.dataforseo.com/v3/keywords_data/google_ads/search_volume/live"
    
    keyword_volumes = {}
    try:
        # st.write(f"Wysy≈Çanie zapytania o wolumeny do DataForSEO dla: {keywords_list}") # Debug
        response = requests.post(endpoint_url, auth=(login, password), headers=headers, json=post_data, timeout=60)
        response.raise_for_status()
        data = response.json()
        # st.write(f"Odpowied≈∫ JSON (wolumeny) od DataForSEO: {data}") # Debug

        if data.get("status_code") == 20000 and data.get("tasks") and data["tasks"][0].get("result"):
            results = data["tasks"][0]["result"]
            for res_item in results:
                keyword = res_item.get("keyword")
                search_volume = res_item.get("search_volume") # Mo≈ºe byƒá None, je≈õli brak danych
                if keyword:
                    keyword_volumes[keyword.lower()] = search_volume if search_volume is not None else "brak danych"
        else:
            status_message = data.get("status_message", "Nieznany b≈ÇƒÖd.")
            tasks_error = ""
            if data.get("tasks") and data["tasks"][0].get("status_message") != "Ok.": # Mo≈ºe byƒá pusta lista tasks lub task[0] mo≈ºe nie mieƒá result
                tasks_error = f" B≈ÇƒÖd zadania: {data['tasks'][0]['status_code']} - {data['tasks'][0]['status_message']}"
            st.warning(f"DataForSEO API (Search Volume) zwr√≥ci≈Ço nieoczekiwany status: {status_message}{tasks_error}.")
        return keyword_volumes
    except Exception as e:
        st.error(f"üõë B≈ÇƒÖd DataForSEO (Search Volume): {e}")
        return {}


@st.cache_data
def scrape_and_clean_content(url_to_scrape, scrapingbee_api_key):
    """Pobiera i czy≈õci tre≈õƒá ze strony u≈ºywajƒÖc ScrapingBee."""
    # (bez zmian)
    try:
        response = requests.get(
            url='https://app.scrapingbee.com/api/v1/',
            params={'api_key': scrapingbee_api_key, 'url': url_to_scrape, 'premium_proxy': 'true', 'block_resources': 'false'},
            timeout=90
        )
        response.raise_for_status()
        extracted_text = extract(response.text, include_comments=False, include_tables=False, include_images=False)
        if not extracted_text: return None
        cleaned_text = re.sub(r'\s+', ' ', extracted_text).strip()
        return cleaned_text if len(cleaned_text) > 100 else None
    except: return None

# --- Funkcje generujƒÖce KA≈ªDƒÑ SEKCJƒò RAPORTU ---
def generate_gemini_response(section_prompt, section_name):
    """Wysy≈Ça pojedynczy prompt do Gemini i zwraca odpowied≈∫."""
    # (bez zmian)
    try:
        model = genai.GenerativeModel('gemini-1.5-flash-latest')
        generation_config = genai.types.GenerationConfig(max_output_tokens=8192)
        response = model.generate_content(section_prompt, generation_config=generation_config)
        if hasattr(response, 'text') and response.text:
            return response.text.strip()
        else:
            st.warning(f"‚ö†Ô∏è Gemini zwr√≥ci≈Ço pustƒÖ odpowied≈∫ dla sekcji: {section_name}.")
            if hasattr(response, 'prompt_feedback'): st.write(f"Feedback dla {section_name}:", response.prompt_feedback)
            if hasattr(response, 'candidates') and response.candidates and response.candidates[0].finish_reason:
                 st.write(f"Przyczyna zako≈Ñczenia dla {section_name}:", response.candidates[0].finish_reason)
            return f"### {section_name}\nBrak danych od AI dla tej sekcji."
    except Exception as e:
        st.error(f"üõë B≈ÇƒÖd komunikacji z Gemini API dla sekcji {section_name}: {e}")
        return f"### {section_name}\nB≈ÇƒÖd generowania tej sekcji."

def generate_kluczowe_punkty(all_content, keyword_phrase):
    # (bez zmian)
    prompt = f"""Jako analityk SEO, przeanalizuj poni≈ºszƒÖ tre≈õƒá z artyku≈Ç√≥w TOP10 dla frazy "{keyword_phrase}".
Twoim zadaniem jest TYLKO wygenerowanie sekcji "### 1. Kluczowe Punkty Wsp√≥lne".
Wypunktuj tematy, podtematy, kluczowe informacje, perspektywy i style narracji, kt√≥re powtarzajƒÖ siƒô w wiƒôkszo≈õci analizowanych tekst√≥w. Skup siƒô na tym, co jest standardem i skonstruuj wytyczne dla copywritera. Odpowied≈∫ musi byƒá TYLKO tre≈õciƒÖ tej sekcji, zaczynajƒÖc od nag≈Ç√≥wka `### 1. Kluczowe Punkty Wsp√≥lne`.

Tre≈õƒá do analizy:
{all_content if all_content else "Brak tre≈õci z artyku≈Ç√≥w TOP10 do analizy."}
"""
    return generate_gemini_response(prompt, "1. Kluczowe Punkty Wsp√≥lne")

def generate_unikalne_elementy(all_content, keyword_phrase):
    # (bez zmian)
    prompt = f"""Jako analityk SEO, przeanalizuj poni≈ºszƒÖ tre≈õƒá z artyku≈Ç√≥w TOP10 dla frazy "{keyword_phrase}".
Twoim zadaniem jest TYLKO wygenerowanie sekcji "### 2. Unikalne i Wyr√≥≈ºniajƒÖce Siƒô Elementy".
Wypunktuj nietypowe, oryginalne, innowacyjne lub szczeg√≥lnie warto≈õciowe informacje, dane, przyk≈Çady, case studies, infografiki (opisz co przedstawiajƒÖ) lub perspektywy, kt√≥re pojawi≈Çy siƒô tylko w niekt√≥rych ≈∫r√≥d≈Çach z TOP10 i mogƒÖ stanowiƒá przewagƒô konkurencyjnƒÖ dla nowego artyku≈Çu. Odpowied≈∫ musi byƒá TYLKO tre≈õciƒÖ tej sekcji, zaczynajƒÖc od nag≈Ç√≥wka `### 2. Unikalne i Wyr√≥≈ºniajƒÖce Siƒô Elementy`.

Tre≈õƒá do analizy:
{all_content if all_content else "Brak tre≈õci z artyku≈Ç√≥w TOP10 do analizy."}
"""
    return generate_gemini_response(prompt, "2. Unikalne i Wyr√≥≈ºniajƒÖce Siƒô Elementy")

def generate_s≈Çowa_kluczowe_initial(all_content, keyword_phrase): # Zmieniono nazwƒô, aby odr√≥≈ºniƒá
    """Generuje WSTƒòPNƒÑ listƒô s≈Ç√≥w kluczowych przez Gemini."""
    prompt = f"""Jako analityk SEO, przeanalizuj poni≈ºszƒÖ tre≈õƒá z artyku≈Ç√≥w TOP10 dla frazy "{keyword_phrase}".
Twoim zadaniem jest TYLKO wygenerowanie sekcji "### 3. Sugerowane S≈Çowa Kluczowe i Semantyka".
Na podstawie analizy tre≈õci konkurencji z TOP10, stw√≥rz listƒô 10-12 najwa≈ºniejszych s≈Ç√≥w kluczowych, fraz d≈Çugoogonowych i pojƒôƒá semantycznie powiƒÖzanych. Pogrupuj je tematycznie, je≈õli to u≈Çatwia zrozumienie. Wska≈º intencjƒô wyszukiwania dla frazy g≈Ç√≥wnej.
Formatuj listƒô s≈Ç√≥w kluczowych jako standardowe punkty Markdown (np. `- S≈Çowo kluczowe`). Nie dodawaj ≈ºadnych dodatkowych informacji poza samymi s≈Çowami kluczowymi i ich ewentualnym grupowaniem tematycznym.
Odpowied≈∫ musi byƒá TYLKO tre≈õciƒÖ tej sekcji, zaczynajƒÖc od nag≈Ç√≥wka `### 3. Sugerowane S≈Çowa Kluczowe i Semantyka`.

Tre≈õƒá do analizy:
{all_content if all_content else "Brak tre≈õci z artyku≈Ç√≥w TOP10 do analizy."}
"""
    return generate_gemini_response(prompt, "3. Sugerowane S≈Çowa Kluczowe i Semantyka (Wstƒôpne)")

def format_s≈Çowa_kluczowe_with_volumes(gemini_section_text, keyword_volumes_map):
    """Dodaje wolumeny do wygenerowanej przez Gemini sekcji s≈Ç√≥w kluczowych."""
    if not gemini_section_text.strip():
        return "### 3. Sugerowane S≈Çowa Kluczowe i Semantyka\nNie uda≈Ço siƒô wygenerowaƒá listy s≈Ç√≥w kluczowych."
    
    lines = gemini_section_text.split('\n')
    output_lines = []
    # Regex do znalezienia linii z punktorem (-, *, lub cyfra.) i tekstem za nim
    keyword_line_pattern = re.compile(r"^\s*[-*]\s+(.+)$|^\s*\d+\.\s+(.+)$")

    for line in lines:
        match = keyword_line_pattern.match(line)
        if match:
            # Bierzemy grupƒô, kt√≥ra nie jest None (dla '-' lub dla '1.')
            keyword_text = next(g for g in match.groups() if g is not None).strip()
            # Usu≈Ñ potencjalne dodatkowe opisy po s≈Çowie kluczowym, je≈õli sƒÖ w tej samej linii i nie sƒÖ czƒô≈õciƒÖ s≈Çowa
            # To jest heurystyka i mo≈ºe wymagaƒá dostosowania
            keyword_to_lookup = keyword_text.split(' (')[0].split(' - ')[0].strip().lower() # Bierzemy tekst przed ' (' lub ' - '

            volume = keyword_volumes_map.get(keyword_to_lookup, "brak danych")
            # Dodajemy tylko je≈õli faktycznie to linia z punktorem i tekstem
            # Je≈õli oryginalna linia mia≈Ça np. pogrubienie, zachowujemy je, dodajƒÖc wolumen
            original_keyword_part_in_line = match.group(0) # Ca≈Ça linia z punktorem
            output_lines.append(f"{original_keyword_part_in_line} (szac. wyszuka≈Ñ/mc: {volume})")
        else:
            output_lines.append(line) # Zachowaj linie, kt√≥re nie sƒÖ s≈Çowami kluczowymi (np. nag≈Ç√≥wki grup)
            
    return "\n".join(output_lines)


def generate_struktura_artykulu(all_content, keyword_phrase):
    # (bez zmian w stosunku do ostatniej wersji)
    prompt = f"""Jako ekspert SEO specjalizujƒÖcy siƒô w tworzeniu BARDZO SZCZEG√ì≈ÅOWYCH i WYCZERPUJƒÑCYCH konspekt√≥w artyku≈Ç√≥w, przeanalizuj poni≈ºszƒÖ tre≈õƒá z artyku≈Ç√≥w TOP10 dla frazy "{keyword_phrase}".
Twoim zadaniem jest TYLKO wygenerowanie sekcji "### 4. Proponowana Struktura Artyku≈Çu (Szkic)".
Zaproponuj niezwykle rozbudowanƒÖ i dog≈ÇƒôbnƒÖ strukturƒô nowego artyku≈Çu w formacie Markdown. Struktura MUSI zawieraƒá:
1.  Co najmniej 2-3 propozycje chwytliwych tytu≈Ç√≥w dla ca≈Çego artyku≈Çu, odpowiednich dla frazy "{keyword_phrase}".
2.  Nastƒôpnie, struktura MUSI byƒá podzielona na **DOK≈ÅADNIE 5 do 6 (piƒôƒá do sze≈õciu) G≈Å√ìWNYCH SEKCJI (ka≈ºda jako nag≈Ç√≥wek H2)**.
3.  Dla **KA≈ªDEJ z tych g≈Ç√≥wnych sekcji H2, MUSISZ zaproponowaƒá **DOK≈ÅADNIE 3 do 4 (trzy do czterech) bardziej szczeg√≥≈Çowych podpunkt√≥w (ka≈ºdy jako nag≈Ç√≥wek H3)**.
Nag≈Ç√≥wki H2 i H3 powinny byƒá anga≈ºujƒÖce, precyzyjnie opisywaƒá zawarto≈õƒá danego fragmentu i naturalnie zawieraƒá s≈Çowa kluczowe, je≈õli to mo≈ºliwe. Dbaj o logiczny przep≈Çyw i kompleksowe pokrycie tematu, czerpiƒÖc inspiracjƒô z analizy TOP10.
Odpowied≈∫ musi byƒá TYLKO tre≈õciƒÖ tej sekcji, zaczynajƒÖc od nag≈Ç√≥wka `### 4. Proponowana Struktura Artyku≈Çu (Szkic)`.

Tre≈õƒá do analizy:
{all_content if all_content else "Brak tre≈õci z artyku≈Ç√≥w TOP10 do analizy."}
"""
    return generate_gemini_response(prompt, "4. Proponowana Struktura Artyku≈Çu (Szkic)")

def generate_faq(all_content, keyword_phrase):
    # (bez zmian)
    prompt = f"""Jako analityk SEO, przeanalizuj poni≈ºszƒÖ tre≈õƒá z artyku≈Ç√≥w TOP10 dla frazy "{keyword_phrase}".
Twoim zadaniem jest TYLKO wygenerowanie sekcji "### 5. Sekcja FAQ (Pytania i Odpowiedzi)".
Stw√≥rz listƒô 4-5 najczƒôstszych pyta≈Ñ, na kt√≥re odpowiadajƒÖ konkurenci z TOP10, w stylu 'People Also Ask'. **Dla ka≈ºdego pytania, podaj 2-3 zdaniowƒÖ bezpo≈õredniƒÖ odpowied≈∫, piszƒÖc jƒÖ BEZPO≈öREDNIO POD DANYM PYTANIEM, w nowej linii.** U≈ºyj formatowania Markdown: pytanie jako zwyk≈Çy tekst lub pogrubiony, a odpowied≈∫ pod nim. Odpowied≈∫ musi byƒá TYLKO tre≈õciƒÖ tej sekcji, zaczynajƒÖc od nag≈Ç√≥wka `### 5. Sekcja FAQ (Pytania i Odpowiedzi)`.

Tre≈õƒá do analizy:
{all_content if all_content else "Brak tre≈õci z artyku≈Ç√≥w TOP10 do analizy."}
"""
    return generate_gemini_response(prompt, "5. Sekcja FAQ (Pytania i Odpowiedzi)")


def parse_report(report_text):
    """Dzieli pe≈Çny raport na sekcje do wy≈õwietlenia w zak≈Çadkach."""
    # (bez zmian)
    if not report_text: return {}
    sections = {}
    pattern = r"###\s*(?:\d+\.\s*)?(.*?)\n(.*?)(?=\n###\s*\d+\.|$|\Z)"
    matches = re.findall(pattern, report_text, re.DOTALL)
    for match in matches:
        title = match[0].strip()
        content = match[1].strip()
        if title and content:
            sections[title] = content
    return sections

# ==============================================================================
# Krok 5: Interfejs U≈ºytkownika i g≈Ç√≥wna logika
# ==============================================================================
keyword = st.text_input("Wprowad≈∫ frazƒô kluczowƒÖ, kt√≥rƒÖ chcesz przeanalizowaƒá:", placeholder="np. jak dbaƒá o buty sk√≥rzane")

if st.button("üöÄ Wygeneruj Kompleksowy Audyt SEO"):
    if not keyword:
        st.warning("Proszƒô wpisaƒá frazƒô kluczowƒÖ.")
        st.stop()

    if 'SCRAPINGBEE_API_KEY' not in st.secrets or \
       'GEMINI_API_KEY' not in st.secrets or \
       'DATAFORSEO_LOGIN' not in st.secrets or \
       'DATAFORSEO_PASSWORD' not in st.secrets:
         st.error("B≈ÇƒÖd: Nie wszystkie wymagane klucze API sƒÖ skonfigurowane w Streamlit Secrets.")
         st.stop()

    with st.spinner("Przeprowadzam pe≈Çny audyt... To mo≈ºe potrwaƒá kilka minut."):
        st.info("Etap 1/4: Pobieranie i filtrowanie wynik√≥w z Google (przez DataForSEO)...")
        top_results = get_serp_data_with_dataforseo(DATAFORSEO_LOGIN, DATAFORSEO_PASSWORD, keyword)
        
        if not top_results:
            st.error(f"Nie uda≈Ço siƒô pobraƒá wynik√≥w organicznych z DataForSEO dla frazy '{keyword}'. Audyt przerwany.")
            st.stop()

        BANNED_DOMAINS = ["youtube.com", "pinterest.", "instagram.com", "facebook.com", "olx.pl", "allegro.pl", "twitter.com", "tiktok.com", "wikipedia.org", "s≈Çownik.pl", "encyklopedia.", "forum.", ".gov", ".edu", "otodom.pl", "gratka.pl", "domiporta.pl"]
        filtered_results = [r for r in top_results if r and r.get('link') and not any(b in r['link'].lower() for b in BANNED_DOMAINS)]

        if not filtered_results:
             st.error("Po filtracji nie pozosta≈Çy ≈ºadne artyku≈Çy do analizy.")
             st.stop()
        
        if len(top_results) > len(filtered_results):
            st.info(f"Pominiƒôto {len(top_results) - len(filtered_results)} wynik√≥w, analizujƒô {len(filtered_results)} artyku≈Ç√≥w.")
        st.subheader("Analizowane adresy URL (po filtracji):")
        for i, r in enumerate(filtered_results, 1): st.write(f"{i}. [{r.get('title', r.get('link'))}]({r.get('link', '#')})")

        all_articles_content_str = ""
        if filtered_results:
            st.info("Etap 2/4: Pobieranie tre≈õci ze stron (ScrapingBee)...")
            all_articles_content_list = []
            progress_bar = st.progress(0)
            for i, result in enumerate(filtered_results):
                url = result.get('link')
                if url:
                    content = scrape_and_clean_content(url, SCRAPINGBEE_API_KEY)
                    if content: all_articles_content_list.append(content)
                    progress_bar.progress((i + 1) / len(filtered_results))
            progress_bar.empty()
            if not all_articles_content_list: st.warning("Nie uda≈Ço siƒô pobraƒá tre≈õci z ≈ºadnej ze stron. Raport bƒôdzie bazowa≈Ç na dostƒôpnych informacjach.")
            else:
                st.success(f"‚úÖ Pomy≈õlnie pobrano tre≈õci z {len(all_articles_content_list)} stron.")
                all_articles_content_str = "\n\n---\n\n".join(all_articles_content_list)
        
        if not all_articles_content_str and not filtered_results:
            st.error("Brak tre≈õci artyku≈Ç√≥w do analizy. Audyt przerwany.")
            st.stop()

        st.info("Etap 3/4: Generowanie raportu przez AI (Gemini) - sekcja po sekcji...")
        report_parts = []
        report_progress = st.progress(0)
        
        # --- ZMIANA TUTAJ: Kolejno≈õƒá i spos√≥b generowania sekcji 3 ---
        sections_definitions = [
            ("1. Kluczowe Punkty Wsp√≥lne", lambda: generate_kluczowe_punkty(all_articles_content_str, keyword)),
            ("2. Unikalne i Wyr√≥≈ºniajƒÖce Siƒô Elementy", lambda: generate_unikalne_elementy(all_articles_content_str, keyword)),
            # Sekcja 3 bƒôdzie teraz generowana wieloetapowo
            ("4. Proponowana Struktura Artyku≈Çu (Szkic)", lambda: generate_struktura_artykulu(all_articles_content_str, keyword)),
            ("5. Sekcja FAQ (Pytania i Odpowiedzi)", lambda: generate_faq(all_articles_content_str, keyword))
        ]
        total_sections_for_progress = len(sections_definitions) + 1 # +1 dla specjalnej obs≈Çugi sekcji 3

        # Generowanie sekcji 1 i 2
        for i in range(2): # Pierwsze dwie sekcje
            section_title, generation_func = sections_definitions[i]
            st.write(f"Generowanie sekcji: {section_title}...")
            part = generation_func()
            report_parts.append(part)
            report_progress.progress((i + 1) / total_sections_for_progress)
            time.sleep(0.2)

        # Etap specjalny: Generowanie sekcji 3 (S≈Çowa kluczowe z wolumenami)
        st.write("Generowanie sekcji: 3. Sugerowane S≈Çowa Kluczowe i Semantyka (krok 1/2 - sugestie AI)...")
        gemini_keywords_section_text = generate_s≈Çowa_kluczowe_initial(all_articles_content_str, keyword)
        report_progress.progress(3 / total_sections_for_progress)
        
        extracted_keywords_for_volume = []
        if gemini_keywords_section_text and "Brak danych" not in gemini_keywords_section_text and "B≈ÇƒÖd generowania" not in gemini_keywords_section_text:
            keyword_line_pattern = re.compile(r"^\s*[-*]\s+(.+)$|^\s*\d+\.\s+(.+)$")
            for line in gemini_keywords_section_text.split('\n'):
                match = keyword_line_pattern.match(line)
                if match:
                    keyword_text = next(g for g in match.groups() if g is not None).strip()
                    # Proste czyszczenie, bierzemy tekst przed ' (' lub ' - '
                    cleaned_keyword = keyword_text.split(' (')[0].split(' - ')[0].strip()
                    if cleaned_keyword: # Upewnij siƒô, ≈ºe co≈õ zosta≈Ço
                        extracted_keywords_for_volume.append(cleaned_keyword)
        
        final_section_3_text = gemini_keywords_section_text # Domy≈õlnie, je≈õli nie ma wolumen√≥w
        if extracted_keywords_for_volume:
            st.write(f"Generowanie sekcji: 3. Sugerowane S≈Çowa Kluczowe i Semantyka (krok 2/2 - pobieranie wolumen√≥w dla {len(extracted_keywords_for_volume)} fraz)...")
            keyword_volumes = get_keyword_volumes_dataforseo(DATAFORSEO_LOGIN, DATAFORSEO_PASSWORD, extracted_keywords_for_volume)
            if keyword_volumes:
                final_section_3_text = format_s≈Çowa_kluczowe_with_volumes(gemini_keywords_section_text, keyword_volumes)
            else:
                st.warning("Nie uda≈Ço siƒô pobraƒá wolumen√≥w wyszukiwa≈Ñ dla sugerowanych s≈Ç√≥w kluczowych.")
        else:
            st.warning("Nie uda≈Ço siƒô wyekstrahowaƒá s≈Ç√≥w kluczowych z sugestii AI do sprawdzenia wolumen√≥w.")

        report_parts.append(final_section_3_text)
        report_progress.progress(4 / total_sections_for_progress) # Aktualizacja postƒôpu po ca≈Çej sekcji 3
        time.sleep(0.2)

        # Generowanie pozosta≈Çych sekcji (4 i 5 z pierwotnej listy)
        for i in range(2, len(sections_definitions)): # Zaczynamy od indeksu 2 (sekcja 4)
            section_title, generation_func = sections_definitions[i]
            st.write(f"Generowanie sekcji: {section_title}...")
            part = generation_func()
            report_parts.append(part)
            report_progress.progress((i + 2) / total_sections_for_progress) # +2 bo 2 ju≈º by≈Çy + 1 za sekcjƒô 3
            time.sleep(0.2)
        
        full_report = "\n\n".join(report_parts)
        report_progress.empty()

        if not full_report or all(p.startswith("###") and "Brak danych" in p or "B≈ÇƒÖd generowania" in p for p in report_parts):
             st.error("Generowanie raportu przez Gemini nie powiod≈Ço siƒô dla ≈ºadnej sekcji lub zwr√≥ci≈Ço b≈Çƒôdy.")
             st.stop()

        st.info("Etap 4/4: Formatowanie wynik√≥w...")
        report_sections = parse_report(full_report)
        
        st.balloons()
        st.success("‚úÖ Audyt SEO gotowy!")
        st.markdown(f"--- \n## Audyt SEO i plan tre≈õci dla frazy: '{keyword}'")

        preferred_tab_order = [
            "Kluczowe Punkty Wsp√≥lne", "Unikalne i Wyr√≥≈ºniajƒÖce Siƒô Elementy",
            "Sugerowane S≈Çowa Kluczowe i Semantyka", # Ten tytu≈Ç bƒôdzie u≈ºyty przez parse_report
            "Proponowana Struktura Artyku≈Çu (Szkic)",
            "Sekcja FAQ (Pytania i Odpowiedzi)"
        ]
        
        actual_tab_titles = [title for title in preferred_tab_order if title in report_sections and report_sections[title].strip()]
        
        if actual_tab_titles:
            tabs = st.tabs(actual_tab_titles)
            for i, tab_title in enumerate(actual_tab_titles):
                with tabs[i]:
                    st.header(tab_title)
                    st.markdown(report_sections[tab_title])
        else:
            st.warning("Brak danych do wy≈õwietlenia w zak≈Çadkach.")
else:
    if keyword: st.info(f"Wprowadzono frazƒô: '{keyword}'. Kliknij przycisk powy≈ºej, aby rozpoczƒÖƒá analizƒô.")
