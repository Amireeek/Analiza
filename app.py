
# -*- coding: utf-8 -*-

# ==============================================================================
# Krok 0: Instalacja bibliotek
# ==============================================================================
# JeÅ›li uruchamiasz to lokalnie, upewnij siÄ™, Å¼e masz te biblioteki zainstalowane:
# pip install streamlit requests trafilatura google-generativeai google-api-python-client scrapingbee
# JeÅ›li uruchamiasz w Streamlit Cloud, dodaj je do pliku requirements.txt

# ==============================================================================
# Krok 1: Import bibliotek
# ==============================================================================
import streamlit as st
import requests
import re
from trafilatura import extract # Upewnij siÄ™, Å¼e masz tÄ™ bibliotekÄ™
import google.generativeai as genai
from googleapiclient.discovery import build # Upewnij siÄ™, Å¼e masz tÄ™ bibliotekÄ™


# ==============================================================================
# Krok 2: Konfiguracja strony Streamlit
# ==============================================================================
st.set_page_config(page_title="SEO Content Powerhouse", page_icon="ğŸš€", layout="wide")
st.title("ğŸš€ SEO Content Powerhouse z AI")
st.markdown("NarzÄ™dzie do tworzenia kompletnych strategii contentowych na podstawie analizy TOP 10 wynikÃ³w Google.")


# ==============================================================================
# Krok 3: ObsÅ‚uga Kluczy API ze Streamlit Secrets
# ==============================================================================
try:
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
    SEARCH_API_KEY = st.secrets["SEARCH_API_KEY"]
    SEARCH_ENGINE_ID = st.secrets["SEARCH_ENGINE_ID"]
    SCRAPINGBEE_API_KEY = st.secrets["SCRAPINGBEE_API_KEY"]
    genai.configure(api_key=GEMINI_API_KEY)
except KeyError as e:
    st.error(f"ğŸ›‘ BÅ‚Ä…d konfiguracji sekretÃ³w! Nie znaleziono wymaganego sekretu: {e}. Upewnij siÄ™, Å¼e skonfigurowaÅ‚eÅ› WSZYSTKIE 4 sekrety (GEMINI_API_KEY, SEARCH_API_KEY, SEARCH_ENGINE_ID, SCRAPINGBEE_API_KEY) w ustawieniach Streamlit.")
    st.stop()
except Exception as e:
    st.error(f"ğŸ›‘ WystÄ…piÅ‚ nieoczekiwany bÅ‚Ä…d podczas Å‚adowania kluczy: {e}")
    st.stop()


# ==============================================================================
# Krok 4: Funkcje Backendowe
# ==============================================================================

@st.cache_data
def get_top_10_results(api_key, cse_id, query):
    """Pobiera 10 najlepszych wynikÃ³w wyszukiwania Google dla danej frazy."""
    try:
        service = build("customsearch", "v1", developerKey=api_key)
        res = service.cse().list(q=query, cx=cse_id, num=10, gl='pl', hl='pl').execute()
        if 'items' not in res or not res['items']:
            return []
        return [{'title': item.get('title'), 'link': item.get('link')} for item in res['items']]
    except Exception as e:
        st.error(f"ğŸ›‘ BÅ‚Ä…d podczas pobierania wynikÃ³w z Google Search API: {e}")
        return None


@st.cache_data
def scrape_and_clean_content(url_to_scrape, scrapingbee_api_key):
    """Pobiera i czyÅ›ci treÅ›Ä‡ ze strony uÅ¼ywajÄ…c ScrapingBee."""
    try:
        response = requests.get(
            url='https://app.scrapingbee.com/api/v1/',
            params={'api_key': scrapingbee_api_key, 'url': url_to_scrape, 'premium_proxy': 'true', 'block_resources': 'false'},
            timeout=90
        )
        response.raise_for_status()
        extracted_text = extract(response.text, include_comments=False, include_tables=False, include_images=False)
        if not extracted_text:
             return None
        cleaned_text = re.sub(r'\s+', ' ', extracted_text).strip()
        return cleaned_text if len(cleaned_text) > 100 else None
    except requests.exceptions.RequestException as e:
        st.warning(f"âš ï¸ Nie udaÅ‚o siÄ™ pobraÄ‡ treÅ›ci z {url_to_scrape} (ScrapingBee): {e}")
        return None
    except Exception as e:
        st.warning(f"âš ï¸ WystÄ…piÅ‚ nieoczekiwany bÅ‚Ä…d podczas przetwarzania treÅ›ci z {url_to_scrape}: {e}")
        return None


@st.cache_data(show_spinner="AI analizuje treÅ›Ä‡...")
def analyze_content_with_gemini(all_content, keyword_phrase):
    """Analizuje zagregowanÄ… treÅ›Ä‡ i generuje raport z Gemini."""
    if not all_content:
        return "Brak treÅ›ci do analizy przez AI."

    # <<< ZMIANA: JESZCZE BARDZIEJ PRECYZYJNY PROMPT >>>
    prompt = f"""
JesteÅ› Å›wiatowej klasy analitykiem SEO i strategiem content marketingu. Twoim zadaniem jest przeanalizowanie dostarczonej treÅ›ci z czoÅ‚owych artykuÅ‚Ã³w dla frazy "{keyword_phrase}" i na tej podstawie wygenerowanie kompleksowego raportu w formacie Markdown.

Twoja odpowiedÅº MUSI byÄ‡ podzielona na DOKÅADNIE 5 sekcji, uÅ¼ywajÄ…c nagÅ‚Ã³wkÃ³w w formacie `### [Numer]. [Nazwa Sekcji]`. Nie uÅ¼ywaj Å¼adnych innych nagÅ‚Ã³wkÃ³w H3 w tytuÅ‚ach sekcji raportu.

### 1. Kluczowe Punkty WspÃ³lne
Wypunktuj tematy, podtematy i kluczowe informacje, ktÃ³re powtarzajÄ… siÄ™ w wiÄ™kszoÅ›ci analizowanych tekstÃ³w. Skup siÄ™ na tym, co jest absolutnym standardem w TOP 10 i musi znaleÅºÄ‡ siÄ™ w nowym artykule.

### 2. Unikalne i WyrÃ³Å¼niajÄ…ce SiÄ™ Elementy
Wypunktuj nietypowe, oryginalne, innowacyjne lub szczegÃ³lnie wartoÅ›ciowe informacje (dane, przykÅ‚ady, case studies, perspektywy), ktÃ³re pojawiÅ‚y siÄ™ tylko w niektÃ³rych ÅºrÃ³dÅ‚ach. To sÄ… elementy, ktÃ³re mogÄ… daÄ‡ przewagÄ™ konkurencyjnÄ….

### 3. Sugerowane SÅ‚owa Kluczowe i Semantyka
StwÃ³rz listÄ™ 10-15 najwaÅ¼niejszych sÅ‚Ã³w kluczowych, fraz dÅ‚ugoogonowych i pojÄ™Ä‡ semantycznie powiÄ…zanych. Pogrupuj je tematycznie. OkreÅ›l intencjÄ™ wyszukiwania dla frazy gÅ‚Ã³wnej (np. informacyjna, komercyjna, transakcyjna).

### 4. Proponowana Struktura ArtykuÅ‚u (Szkic)
To jest najwaÅ¼niejsza sekcja. StwÃ³rz ROZBUDOWANY i KOMPLETNY plan artykuÅ‚u w formacie Markdown. Twoja odpowiedÅº dla tej sekcji MUSI zawieraÄ‡:
1.  Jeden chwytliwy tytuÅ‚ (jako nagÅ‚Ã³wek H1, np. `# TytuÅ‚ ArtykuÅ‚u`).
2.  KrÃ³tki, angaÅ¼ujÄ…cy wstÄ™p (2-3 zdania).
3.  ListÄ™ co najmniej 4-5 gÅ‚Ã³wnych sekcji artykuÅ‚u (jako nagÅ‚Ã³wki H2, np. `## NagÅ‚Ã³wek Sekcji GÅ‚Ã³wnej`).
4.  Dla kaÅ¼dej sekcji H2, zaproponuj 2-4 podpunkty lub tematy do omÃ³wienia (jako nagÅ‚Ã³wki H3, np. `### Podpunkt w Sekcji`).
Struktura musi byÄ‡ logiczna i wyczerpujÄ…ca.

### 5. Sekcja FAQ (Pytania i Odpowiedzi)
StwÃ³rz listÄ™ 4-5 najczÄ™stszych pytaÅ„ w stylu 'People Also Ask'. Pod kaÅ¼dym pytaniem podaj zwiÄ™zÅ‚Ä…, 2-3 zdaniowÄ… odpowiedÅº opartÄ… na przeanalizowanej treÅ›ci.

PamiÄ™taj, aby Twoja odpowiedÅº byÅ‚a TYLKO treÅ›ciÄ… raportu w formacie Markdown, bez Å¼adnych dodatkowych wstÄ™pÃ³w czy podsumowaÅ„ poza wymaganÄ… strukturÄ…. CaÅ‚a odpowiedÅº musi byÄ‡ w jÄ™zyku polskim.
TreÅ›Ä‡ do analizy:
{all_content}
"""
    # === KONIEC ZMODYFIKOWANEGO PROMPTU ===

    try:
        model = genai.GenerativeModel('gemini-1.5-flash-latest')
        response = model.generate_content(prompt)
        if hasattr(response, 'text') and response.text:
             return response.text
        else:
             st.warning("âš ï¸ Gemini zwrÃ³ciÅ‚o pustÄ… odpowiedÅº lub bÅ‚Ä…d. SzczegÃ³Å‚y poniÅ¼ej.")
             if hasattr(response, 'prompt_feedback'):
                 st.write("Feedback z promptu:", response.prompt_feedback)
             if hasattr(response, 'candidates') and response.candidates and response.candidates[0].finish_reason:
                 st.write("Przyczyna zakoÅ„czenia generacji przez API:", response.candidates[0].finish_reason)
             return None
    except Exception as e:
        st.error(f"ğŸ›‘ BÅ‚Ä…d podczas komunikacji z Gemini API: {e}")
        return None


# <<< ZMIANA KRYTYCZNA: POPRAWIONY PARSER >>>
def parse_report(report_text):
    """Dzieli peÅ‚ny raport na sekcje, aby uniknÄ…Ä‡ kolizji z nagÅ‚Ã³wkami H3 w szkicu."""
    if not report_text: return {}
    sections = {}
    # WyraÅ¼enie regularne szuka teraz nagÅ‚Ã³wka sekcji, ktÃ³ry MUSI zaczynaÄ‡ siÄ™ od ###, spacji, cyfry i kropki.
    # DziÄ™ki temu ignoruje nagÅ‚Ã³wki H3 (`### Tekst`) wewnÄ…trz szkicu artykuÅ‚u.
    pattern = r"###\s*(\d+\.\s*.*?)\n(.*?)(?=\n###\s*\d+\.|$)"

    matches = re.findall(pattern, report_text, re.DOTALL | re.MULTILINE)

    for match in matches:
        # TytuÅ‚ teraz zawiera numer, np. "1. Kluczowe Punkty WspÃ³lne"
        title = match[0].strip()
        content = match[1].strip()
        if title:
            # Usuwamy numeracjÄ™ z klucza sÅ‚ownika dla spÃ³jnoÅ›ci z listÄ… preferred_tab_order
            clean_title = re.sub(r"^\d+\.\s*", "", title)
            sections[clean_title] = content

    # JeÅ›li parser nic nie znalazÅ‚ (bo np. AI nie uÅ¼yÅ‚o numeracji), sprÃ³buj starej metody jako fallback
    if not sections:
        pattern_fallback = r"###\s*(.*?)\n(.*?)(?=\n###\s*|$)"
        matches_fallback = re.findall(pattern_fallback, report_text, re.DOTALL | re.MULTILINE)
        for match in matches_fallback:
            title = match[0].strip()
            content = match[1].strip()
            if title and not title.startswith('#'): # Dodatkowe zabezpieczenie
                sections[title] = content

    return sections


# ==============================================================================
# Krok 5: Interfejs UÅ¼ytkownika Streamlit i gÅ‚Ã³wna logika
# ==============================================================================

keyword = st.text_input("WprowadÅº frazÄ™ kluczowÄ…, ktÃ³rÄ… chcesz przeanalizowaÄ‡:", placeholder="np. jak dbaÄ‡ o buty skÃ³rzane")

if st.button("ğŸš€ Wygeneruj Kompleksowy Audyt SEO"):
    if not keyword:
        st.warning("ProszÄ™ wpisaÄ‡ frazÄ™ kluczowÄ….")
        st.stop()

    if not all(k in st.secrets for k in ["GEMINI_API_KEY", "SEARCH_API_KEY", "SEARCH_ENGINE_ID", "SCRAPINGBEE_API_KEY"]):
         st.error("BÅ‚Ä…d: Nie wszystkie klucze API sÄ… skonfigurowane w Streamlit Secrets.")
         st.stop()

    with st.spinner("Przeprowadzam peÅ‚ny audyt... To moÅ¼e potrwaÄ‡ kilka minut."):
        st.info("Etap 1/4: Pobieranie i filtrowanie wynikÃ³w z Google...")
        top_results = get_top_10_results(SEARCH_API_KEY, SEARCH_ENGINE_ID, keyword)
        if not top_results:
            st.error(f"Nie znaleziono Å¼adnych wynikÃ³w TOP 10 dla frazy: '{keyword}'.")
            st.stop()

        BANNED_DOMAINS = ["youtube.com", "pinterest.", "instagram.com", "facebook.com", "olx.pl", "allegro.pl", "twitter.com", "tiktok.com", "wikipedia.org", ".gov", ".edu"]
        filtered_results = [r for r in top_results if r and r.get('link') and not any(b in r['link'].lower() for b in BANNED_DOMAINS)]

        if not filtered_results:
            st.error("Po filtracji nie pozostaÅ‚y Å¼adne artykuÅ‚y do analizy.")
            st.stop()

        st.info(f"PominiÄ™to {len(top_results) - len(filtered_results)} wynikÃ³w, analizujÄ™ {len(filtered_results)} znalezionych artykuÅ‚Ã³w.")
        with st.expander("Zobacz analizowane adresy URL"):
            for i, result in enumerate(filtered_results, 1):
                display_title = result.get('title', result.get('link', 'Brak tytuÅ‚u'))
                st.write(f"{i}. [{display_title}]({result.get('link', '#')})")

        st.info("Etap 2/4: Pobieranie treÅ›ci ze stron...")
        all_articles_content, successful_sources = [], []
        progress_bar = st.progress(0, text="Pobieranie...")
        for i, result in enumerate(filtered_results):
             url = result.get('link')
             if url:
                 content = scrape_and_clean_content(url, SCRAPINGBEE_API_KEY)
                 if content:
                     all_articles_content.append(content)
                     successful_sources.append({'title': result.get('title', 'Brak tytuÅ‚u'), 'link': url})
                 progress_bar.progress((i + 1) / len(filtered_results), text=f"Pobrano {i+1}/{len(filtered_results)}")
        progress_bar.empty()

        if not all_articles_content:
            st.error("Nie udaÅ‚o siÄ™ pobraÄ‡ treÅ›ci z Å¼adnej ze stron.")
            st.stop()
        st.success(f"âœ… PomyÅ›lnie pobrano treÅ›ci z {len(all_articles_content)} stron.")

        average_word_count = sum(len(text.split()) for text in all_articles_content) // len(all_articles_content) if all_articles_content else 0

        st.info("Etap 3/4: Generowanie kompleksowego raportu przez AI...")
        aggregated_content = "\n\n---\n\n".join(all_articles_content)
        full_report = analyze_content_with_gemini(aggregated_content, keyword)

        if not full_report:
             st.error("Generowanie raportu przez Gemini nie powiodÅ‚o siÄ™.")
             st.stop()

        st.info("Etap 4/4: Formatowanie wynikÃ³w...")
        report_sections = parse_report(full_report)

        sources_text = "\n".join([f"- [{source['title']}]({source['link']})" for source in successful_sources])
        report_sections["Analizowane Å¹rÃ³dÅ‚a"] = "PoniÅ¼ej lista adresÃ³w URL, ktÃ³rych treÅ›Ä‡ zostaÅ‚a pomyÅ›lnie pobrana i przeanalizowana przez AI:\n" + sources_text

        st.balloons()
        st.success("âœ… Audyt SEO gotowy!")
        st.markdown(f"--- \n## Audyt SEO i plan treÅ›ci dla frazy: '{keyword}'")

        if average_word_count > 0:
            st.metric(label="Åšrednia dÅ‚ugoÅ›Ä‡ analizowanych artykuÅ‚Ã³w", value=f"~ {average_word_count} sÅ‚Ã³w")
            st.markdown("---")

        preferred_tab_order = ["Kluczowe Punkty WspÃ³lne", "Unikalne i WyrÃ³Å¼niajÄ…ce SiÄ™ Elementy", "Sugerowane SÅ‚owa Kluczowe i Semantyka", "Proponowana Struktura ArtykuÅ‚u (Szkic)", "Sekcja FAQ (Pytania i Odpowiedzi)", "Analizowane Å¹rÃ³dÅ‚a"]
        
        # <<< ZMIANA: Lepsze dopasowanie nazw zakÅ‚adek do tego, co zwraca parser >>>
        # Sprawdzamy, czy klucze ze sÅ‚ownika `report_sections` pasujÄ… do `preferred_tab_order`
        actual_tab_titles = [title for title in preferred_tab_order if title in report_sections and report_sections[title].strip()]
        
        # JeÅ›li parser zwrÃ³ciÅ‚ klucze z numeracjÄ… (np. "1. Kluczowe..."), a w preferred_tab_order mamy bez, to by nie zadziaÅ‚aÅ‚o.
        # Dlatego parser teraz usuwa numeracjÄ™ z klucza.

        if actual_tab_titles:
             tabs = st.tabs(actual_tab_titles)
             for i, tab in enumerate(tabs):
                 with tab:
                     current_title = actual_tab_titles[i]
                     st.header(current_title)
                     st.markdown(report_sections[current_title], unsafe_allow_html=True) # Dodano unsafe_allow_html dla pewnoÅ›ci
        elif report_sections:
             st.warning("Nie udaÅ‚o siÄ™ dopasowaÄ‡ sekcji raportu do zakÅ‚adek. WyÅ›wietlam caÅ‚y raport poniÅ¼ej.")
             st.markdown(full_report)
        else:
             st.warning("Brak danych do wyÅ›wietlenia. OdpowiedÅº z AI mogÅ‚a byÄ‡ pusta lub w nieprawidÅ‚owym formacie.")


else:
    if keyword:
         st.info(f"Wprowadzono frazÄ™: '{keyword}'. Kliknij przycisk powyÅ¼ej, aby rozpoczÄ…Ä‡ analizÄ™.")

Krok 2: Jak wymusiÄ‡ odÅ›wieÅ¼enie analizy

Teraz, gdy kod jest poprawiony, musisz upewniÄ‡ siÄ™, Å¼e Streamlit nie uÅ¼yje starego wyniku. Masz dwie opcje:

Opcja A (Najprostsza): UÅ¼yj innej frazy kluczowej.
Wpisz w pole do analizy frazÄ™, ktÃ³rej jeszcze nie uÅ¼ywaÅ‚eÅ›, np. "najlepszy ekspres do kawy" zamiast "jak dbaÄ‡ o buty". PoniewaÅ¼ argumenty funkcji analyze_content_with_gemini bÄ™dÄ… inne (keyword_phrase siÄ™ zmieni), Streamlit bÄ™dzie musiaÅ‚ wykonaÄ‡ jÄ… od nowa.

Opcja B (Uniwersalna): WyczyÅ›Ä‡ pamiÄ™Ä‡ podrÄ™cznÄ… (cache).

Uruchom aplikacjÄ™.

W prawym gÃ³rnym rogu okna aplikacji kliknij na ikonÄ™ "hamburgera" (trzy poziome kreski).

Wybierz z menu opcjÄ™ "Clear cache".

Po wyczyszczeniu cache'u uruchom analizÄ™ dla dowolnej frazy (nawet tej samej co wczeÅ›niej). Aplikacja bÄ™dzie zmuszona wykonaÄ‡ wszystkie obliczenia od nowa, uÅ¼ywajÄ…c juÅ¼ poprawionego kodu.

Po wykonaniu tych dwÃ³ch krokÃ³w (aktualizacja kodu i odÅ›wieÅ¼enie cache/zmiana frazy), powinieneÅ› otrzymaÄ‡ znacznie bardziej rozbudowanÄ… i zgodnÄ… z oczekiwaniami strukturÄ™ artykuÅ‚u.
