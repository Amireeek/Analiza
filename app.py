# -*- coding: utf-8 -*-

# ==============================================================================
# Krok 0: Instalacja bibliotek
# ==============================================================================
# pip install streamlit requests trafilatura google-generativeai scrapingbee pandas tabulate
# ==============================================================================
# Krok 1: Import bibliotek
# ==============================================================================
import streamlit as st
import requests
import re
from trafilatura import extract
import google.generativeai as genai
from urllib.parse import urlencode as encode_query_params
import json
import time
import pandas as pd

# ==============================================================================
# Krok 2: Konfiguracja strony Streamlit
# ==============================================================================
st.set_page_config(page_title="SEO Content Powerhouse", page_icon="üöÄ", layout="wide")
st.title("üöÄ SEO Content Powerhouse z AI")
st.markdown("Narzƒôdzie do tworzenia kompletnych strategii contentowych na podstawie analizy TOP 10 wynik√≥w Google.")

# ==============================================================================
# Krok 3: Obs≈Çuga Kluczy API ze Streamlit Secrets
# ==============================================================================
try:
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
    SCRAPINGBEE_API_KEY = st.secrets["SCRAPINGBEE_API_KEY"]
    DATAFORSEO_LOGIN = st.secrets["DATAFORSEO_LOGIN"]
    DATAFORSEO_PASSWORD = st.secrets["DATAFORSEO_PASSWORD"]
    genai.configure(api_key=GEMINI_API_KEY)
except KeyError as e:
    missing_key = str(e).strip("'")
    st.error(f"üõë B≈ÇƒÖd konfiguracji sekret√≥w! Nie znaleziono wymaganego sekretu: {missing_key}.")
    st.stop()
except Exception as e:
    st.error(f"üõë WystƒÖpi≈Ç nieoczekiwany b≈ÇƒÖd podczas ≈Çadowania kluczy: {e}")
    st.stop()

# ==============================================================================
# Krok 4: Funkcje Backendowe
# ==============================================================================

@st.cache_data
def get_serp_data_with_dataforseo(login, password, query, num_results=10, location_code=2616, language_code='pl'):
    # (bez zmian)
    post_data = [{"keyword": query, "location_code": location_code, "language_code": language_code, "depth": num_results}]
    headers = {'Content-Type': 'application/json'}
    endpoint_url = "https://api.dataforseo.com/v3/serp/google/organic/live/regular"
    organic_results_list = []
    try:
        response = requests.post(endpoint_url, auth=(login, password), headers=headers, json=post_data, timeout=60)
        response.raise_for_status()
        data = response.json()
        if data.get("status_code") == 20000 and data.get("tasks") and data["tasks"][0].get("result") and data["tasks"][0]["result"][0].get("items"):
            items = data["tasks"][0]["result"][0]["items"]
            for item in items:
                if item.get("type") == "organic":
                    title, link = item.get("title"), item.get("url")
                    if title and link: organic_results_list.append({'title': title, 'link': link})
        else:
            status_message = data.get("status_message", "Nieznany b≈ÇƒÖd.")
            tasks_error = ""
            if data.get("tasks") and data["tasks"][0].get("status_message") != "Ok.":
                tasks_error = f" B≈ÇƒÖd zadania: {data['tasks'][0]['status_code']} - {data['tasks'][0]['status_message']}"
            st.warning(f"DataForSEO API (SERP) zwr√≥ci≈Ço nieoczekiwany status lub brak wynik√≥w: {status_message}{tasks_error}.")
        return organic_results_list
    except Exception as e:
        st.error(f"üõë B≈ÇƒÖd DataForSEO (SERP): {e}")
        return []

def clean_keyword_for_dataforseo(keyword):
    """Usuwa lub zastƒôpuje znaki nieakceptowane przez DataForSEO Search Volume API."""
    # Usuwamy nawiasy i ich zawarto≈õƒá, a tak≈ºe znaki specjalne, pozostawiajƒÖc spacje i alfanumeryczne
    # To jest prosta implementacja, mo≈ºe wymagaƒá dostosowania
    cleaned = re.sub(r'\(.*?\)', '', keyword) # Usu≈Ñ (co≈õ)
    cleaned = re.sub(r'[^\w\sƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈ºƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª-]', '', cleaned, flags=re.UNICODE) # Pozostaw litery, cyfry, spacje, my≈õlniki
    return cleaned.strip()

@st.cache_data
def get_keyword_volumes_dataforseo(login, password, keywords_list, location_code=2616, language_code='pl'):
    if not keywords_list: return {}
    
    # --- ZMIANA: Czyszczenie s≈Ç√≥w kluczowych PRZED wys≈Çaniem do API ---
    # Tworzymy mapowanie oryginalnego s≈Çowa na oczyszczone, aby potem m√≥c je powiƒÖzaƒá z wolumenem
    original_to_cleaned_map = {kw: clean_keyword_for_dataforseo(kw) for kw in keywords_list if kw}
    # Bierzemy tylko unikalne, oczyszczone s≈Çowa, kt√≥re nie sƒÖ puste
    unique_cleaned_keywords = list(set(ckw for ckw in original_to_cleaned_map.values() if ckw))

    if not unique_cleaned_keywords:
        st.write("DEBUG: Lista unikalnych, oczyszczonych s≈Ç√≥w kluczowych jest pusta.")
        return {}

    chunk_size = 100
    keyword_chunks = [unique_cleaned_keywords[i:i + chunk_size] for i in range(0, len(unique_cleaned_keywords), chunk_size)]
    
    # Ten s≈Çownik bƒôdzie przechowywa≈Ç wolumeny dla OCZYSZCZONYCH s≈Ç√≥w kluczowych
    cleaned_keyword_volumes = {} 
    headers = {'Content-Type': 'application/json'}
    endpoint_url = "https://api.dataforseo.com/v3/keywords_data/google_ads/search_volume/live"
    
    st.write(f"DEBUG: Bƒôdƒô sprawdzaƒá wolumeny dla {len(unique_cleaned_keywords)} unikalnych oczyszczonych s≈Ç√≥w w {len(keyword_chunks)} paczkach.")

    for chunk_index, chunk in enumerate(keyword_chunks):
        post_data = [{"keywords": chunk, "location_code": location_code, "language_code": language_code}]
        try:
            st.write(f"DEBUG: Wysy≈Çanie paczki {chunk_index + 1}/{len(keyword_chunks)} o wolumeny (oczyszczone): {chunk}")
            response = requests.post(endpoint_url, auth=(login, password), headers=headers, json=post_data, timeout=60)
            response.raise_for_status()
            data = response.json()
            st.write(f"DEBUG: Odpowied≈∫ JSON (wolumeny, paczka {chunk_index + 1}) od DataForSEO: {data}")

            if data.get("status_code") == 20000 and data.get("tasks") and data["tasks"][0].get("result"):
                results = data["tasks"][0]["result"]
                for res_item in results:
                    keyword_from_api = res_item.get("keyword") # To bƒôdzie oczyszczone s≈Çowo
                    search_volume = res_item.get("search_volume")
                    if keyword_from_api:
                        cleaned_keyword_volumes[keyword_from_api.lower()] = search_volume if search_volume is not None else "brak danych"
            else: # B≈ÇƒÖd na poziomie zadania DataForSEO
                status_message = data.get("status_message", "Nieznany b≈ÇƒÖd.")
                tasks_error = ""
                if data.get("tasks") and data["tasks"][0].get("status_code") != 20000:
                    tasks_error = f" B≈ÇƒÖd zadania: {data['tasks'][0]['status_code']} - {data['tasks'][0]['status_message']}"
                    # Je≈õli b≈ÇƒÖd dotyczy nieprawid≈Çowych znak√≥w, oznaczamy wszystkie w paczce
                    if data['tasks'][0].get('status_code') == 40501: # Invalid Field
                         for kw_in_chunk in chunk:
                            cleaned_keyword_volumes[kw_in_chunk.lower()] = "nieprawid≈Çowe znaki"
                st.warning(f"DataForSEO API (Search Volume, paczka {chunk_index + 1}) status: {status_message}{tasks_error}.")
        except Exception as e:
            st.error(f"üõë B≈ÇƒÖd DataForSEO (Search Volume, paczka {chunk_index + 1}): {e}")
            for kw_in_chunk in chunk:
                cleaned_keyword_volumes[kw_in_chunk.lower()] = "b≈ÇƒÖd pobierania"
        time.sleep(0.3)

    # --- ZMIANA: Mapowanie wolumen√≥w z powrotem na ORYGINALNE s≈Çowa kluczowe ---
    final_volumes_for_original_keywords = {}
    for original_kw, cleaned_kw in original_to_cleaned_map.items():
        final_volumes_for_original_keywords[original_kw] = cleaned_keyword_volumes.get(cleaned_kw.lower(), "brak danych (po czyszczeniu)")
    
    st.write(f"DEBUG: Ko≈Ñcowe wolumeny dla oryginalnych s≈Ç√≥w: {final_volumes_for_original_keywords}")
    return final_volumes_for_original_keywords


@st.cache_data
def scrape_and_clean_content(url_to_scrape, scrapingbee_api_key):
    # (bez zmian)
    try:
        response = requests.get(
            url='https://app.scrapingbee.com/api/v1/',
            params={'api_key': scrapingbee_api_key, 'url': url_to_scrape, 'premium_proxy': 'true', 'block_resources': 'false'},
            timeout=90
        )
        response.raise_for_status(); extracted_text = extract(response.text, include_comments=False, include_tables=False, include_images=False)
        if not extracted_text: return None
        cleaned_text = re.sub(r'\s+', ' ', extracted_text).strip()
        return cleaned_text if len(cleaned_text) > 100 else None
    except: return None

def generate_gemini_response(section_prompt, section_name):
    # (bez zmian)
    try:
        model = genai.GenerativeModel('gemini-1.5-flash-latest')
        generation_config = genai.types.GenerationConfig(max_output_tokens=8192)
        response = model.generate_content(section_prompt, generation_config=generation_config)
        if hasattr(response, 'text') and response.text: return response.text.strip()
        else:
            st.warning(f"‚ö†Ô∏è Gemini pusty wynik dla: {section_name}. Feedback: {getattr(response, 'prompt_feedback', 'N/A')}, Zako≈Ñczenie: {getattr(response.candidates[0] if response.candidates else None, 'finish_reason', 'N/A')}")
            return f"### {section_name}\nBrak danych od AI."
    except Exception as e:
        st.error(f"üõë B≈ÇƒÖd Gemini dla {section_name}: {e}")
        return f"### {section_name}\nB≈ÇƒÖd generowania."

def generate_kluczowe_punkty(all_content, keyword_phrase):
    # (bez zmian)
    prompt = f"""Jako analityk SEO, przeanalizuj poni≈ºszƒÖ tre≈õƒá z artyku≈Ç√≥w TOP10 dla frazy "{keyword_phrase}". Twoim zadaniem jest TYLKO wygenerowanie sekcji "### 1. Kluczowe Punkty Wsp√≥lne". Wypunktuj tematy, podtematy, kluczowe informacje, perspektywy i style narracji, kt√≥re powtarzajƒÖ siƒô w wiƒôkszo≈õci analizowanych tekst√≥w. Skup siƒô na tym, co jest standardem i skonstruuj wytyczne dla copywritera. Odpowied≈∫ musi byƒá TYLKO tre≈õciƒÖ tej sekcji, zaczynajƒÖc od nag≈Ç√≥wka `### 1. Kluczowe Punkty Wsp√≥lne`. Tre≈õƒá do analizy:\n{all_content if all_content else "Brak tre≈õci z artyku≈Ç√≥w TOP10 do analizy."}"""
    return generate_gemini_response(prompt, "1. Kluczowe Punkty Wsp√≥lne")

def generate_unikalne_elementy(all_content, keyword_phrase):
    # (bez zmian)
    prompt = f"""Jako analityk SEO, przeanalizuj poni≈ºszƒÖ tre≈õƒá z artyku≈Ç√≥w TOP10 dla frazy "{keyword_phrase}". Twoim zadaniem jest TYLKO wygenerowanie sekcji "### 2. Unikalne i Wyr√≥≈ºniajƒÖce Siƒô Elementy". Wypunktuj nietypowe, oryginalne, innowacyjne lub szczeg√≥lnie warto≈õciowe informacje, dane, przyk≈Çady, case studies, infografiki (opisz co przedstawiajƒÖ) lub perspektywy, kt√≥re pojawi≈Çy siƒô tylko w niekt√≥rych ≈∫r√≥d≈Çach z TOP10 i mogƒÖ stanowiƒá przewagƒô konkurencyjnƒÖ dla nowego artyku≈Çu. Odpowied≈∫ musi byƒá TYLKO tre≈õciƒÖ tej sekcji, zaczynajƒÖc od nag≈Ç√≥wka `### 2. Unikalne i Wyr√≥≈ºniajƒÖce Siƒô Elementy`. Tre≈õƒá do analizy:\n{all_content if all_content else "Brak tre≈õci z artyku≈Ç√≥w TOP10 do analizy."}"""
    return generate_gemini_response(prompt, "2. Unikalne i Wyr√≥≈ºniajƒÖce Siƒô Elementy")

def generate_s≈Çowa_kluczowe_initial_for_table(all_content, keyword_phrase):
    """Generuje WSTƒòPNƒÑ listƒô s≈Ç√≥w kluczowych przez Gemini, z my≈õlƒÖ o tabeli."""
    # (bez zmian)
    prompt = f"""Jako analityk SEO, przeanalizuj poni≈ºszƒÖ tre≈õƒá z artyku≈Ç√≥w TOP10 dla frazy "{keyword_phrase}". Twoim zadaniem jest TYLKO wygenerowanie sekcji "### 3. Sugerowane S≈Çowa Kluczowe i Semantyka". Na podstawie analizy tre≈õci konkurencji z TOP10, stw√≥rz listƒô 10-15 najwa≈ºniejszych s≈Ç√≥w kluczowych i fraz d≈Çugoogonowych. Mo≈ºesz je pogrupowaƒá tematycznie, u≈ºywajƒÖc nag≈Ç√≥wk√≥w H4 (#### Nazwa Grupy) je≈õli to konieczne. Ka≈ºde s≈Çowo kluczowe lub fraza powinno byƒá w osobnej linii, poprzedzone my≈õlnikiem (np. `- Moje s≈Çowo kluczowe`). Nie dodawaj ≈ºadnych dodatkowych opis√≥w ani intencji wyszukiwania bezpo≈õrednio przy s≈Çowach kluczowych na li≈õcie. Zachowaj czystƒÖ listƒô fraz. Wska≈º og√≥lnƒÖ intencjƒô wyszukiwania dla frazy g≈Ç√≥wnej "{keyword_phrase}" w jednym zdaniu na ko≈Ñcu sekcji, po li≈õcie s≈Ç√≥w. Odpowied≈∫ musi byƒá TYLKO tre≈õciƒÖ tej sekcji, zaczynajƒÖc od nag≈Ç√≥wka `### 3. Sugerowane S≈Çowa Kluczowe i Semantyka`. Tre≈õƒá do analizy:\n{all_content if all_content else "Brak tre≈õci z artyku≈Ç√≥w TOP10 do analizy."}"""
    return generate_gemini_response(prompt, "3. Sugerowane S≈Çowa Kluczowe i Semantyka (Wstƒôpne)")

def generate_struktura_artykulu(all_content, keyword_phrase):
    # (bez zmian)
    prompt = f"""Jako ekspert SEO specjalizujƒÖcy siƒô w tworzeniu BARDZO SZCZEG√ì≈ÅOWYCH i WYCZERPUJƒÑCYCH konspekt√≥w artyku≈Ç√≥w, przeanalizuj poni≈ºszƒÖ tre≈õƒá z artyku≈Ç√≥w TOP10 dla frazy "{keyword_phrase}". Twoim zadaniem jest TYLKO wygenerowanie sekcji "### 4. Proponowana Struktura Artyku≈Çu (Szkic)". Zaproponuj niezwykle rozbudowanƒÖ i dog≈ÇƒôbnƒÖ strukturƒô nowego artyku≈Çu w formacie Markdown. Struktura MUSI zawieraƒá: 1. Co najmniej 2-3 propozycje chwytliwych tytu≈Ç√≥w dla ca≈Çego artyku≈Çu, odpowiednich dla frazy "{keyword_phrase}". 2. Nastƒôpnie, struktura MUSI byƒá podzielona na **DOK≈ÅADNIE 5 do 6 (piƒôƒá do sze≈õciu) G≈Å√ìWNYCH SEKCJI (ka≈ºda jako nag≈Ç√≥wek H2)**. 3. Dla **KA≈ªDEJ z tych g≈Ç√≥wnych sekcji H2, MUSISZ zaproponowaƒá **DOK≈ÅADNIE 3 do 4 (trzy do czterech) bardziej szczeg√≥≈Çowych podpunkt√≥w (ka≈ºdy jako nag≈Ç√≥wek H3)**. Nag≈Ç√≥wki H2 i H3 powinny byƒá anga≈ºujƒÖce, precyzyjnie opisywaƒá zawarto≈õƒá danego fragmentu i naturalnie zawieraƒá s≈Çowa kluczowe, je≈õli to mo≈ºliwe. Dbaj o logiczny przep≈Çyw i kompleksowe pokrycie tematu, czerpiƒÖc inspiracjƒô z analizy TOP10. Odpowied≈∫ musi byƒá TYLKO tre≈õciƒÖ tej sekcji, zaczynajƒÖc od nag≈Ç√≥wka `### 4. Proponowana Struktura Artyku≈Çu (Szkic)`. Tre≈õƒá do analizy:\n{all_content if all_content else "Brak tre≈õci z artyku≈Ç√≥w TOP10 do analizy."}"""
    return generate_gemini_response(prompt, "4. Proponowana Struktura Artyku≈Çu (Szkic)")

def generate_faq(all_content, keyword_phrase):
    # (bez zmian)
    prompt = f"""Jako analityk SEO, przeanalizuj poni≈ºszƒÖ tre≈õƒá z artyku≈Ç√≥w TOP10 dla frazy "{keyword_phrase}". Twoim zadaniem jest TYLKO wygenerowanie sekcji "### 5. Sekcja FAQ (Pytania i Odpowiedzi)". Stw√≥rz listƒô 4-5 najczƒôstszych pyta≈Ñ, na kt√≥re odpowiadajƒÖ konkurenci z TOP10, w stylu 'People Also Ask'. **Dla ka≈ºdego pytania, podaj 2-3 zdaniowƒÖ bezpo≈õredniƒÖ odpowied≈∫, piszƒÖc jƒÖ BEZPO≈öREDNIO POD DANYM PYTANIEM, w nowej linii.** U≈ºyj formatowania Markdown: pytanie jako zwyk≈Çy tekst lub pogrubiony, a odpowied≈∫ pod nim. Odpowied≈∫ musi byƒá TYLKO tre≈õciƒÖ tej sekcji, zaczynajƒÖc od nag≈Ç√≥wka `### 5. Sekcja FAQ (Pytania i Odpowiedzi)`. Tre≈õƒá do analizy:\n{all_content if all_content else "Brak tre≈õci z artyku≈Ç√≥w TOP10 do analizy."}"""
    return generate_gemini_response(prompt, "5. Sekcja FAQ (Pytania i Odpowiedzi)")

def parse_report(report_text):
    # (bez zmian)
    if not report_text: return {}
    sections = {}
    pattern = r"###\s*(?:\d+\.\s*)?(.*?)\n(.*?)(?=\n###\s*\d+\.|$|\Z)"
    matches = re.findall(pattern, report_text, re.DOTALL)
    for match in matches:
        title, content = match[0].strip(), match[1].strip()
        if title and content: sections[title] = content
    return sections

# ==============================================================================
# Krok 5: Interfejs U≈ºytkownika i g≈Ç√≥wna logika
# ==============================================================================
# (Zmiany g≈Ç√≥wnie w sposobie generowania i sk≈Çadania sekcji 3)
keyword = st.text_input("Wprowad≈∫ frazƒô kluczowƒÖ, kt√≥rƒÖ chcesz przeanalizowaƒá:", placeholder="np. jak dbaƒá o buty sk√≥rzane")

if st.button("üöÄ Wygeneruj Kompleksowy Audyt SEO"):
    if not keyword: st.warning("Proszƒô wpisaƒá frazƒô kluczowƒÖ."); st.stop()
    if not all(k in st.secrets for k in ["SCRAPINGBEE_API_KEY", "GEMINI_API_KEY", "DATAFORSEO_LOGIN", "DATAFORSEO_PASSWORD"]):
        st.error("B≈ÇƒÖd: Nie wszystkie wymagane klucze API sƒÖ skonfigurowane."); st.stop()

    with st.spinner("Przeprowadzam pe≈Çny audyt..."):
        st.info("Etap 1/4: Pobieranie wynik√≥w z Google (DataForSEO)...")
        top_results = get_serp_data_with_dataforseo(DATAFORSEO_LOGIN, DATAFORSEO_PASSWORD, keyword)
        if not top_results: st.error(f"Nie uda≈Ço siƒô pobraƒá wynik√≥w organicznych dla '{keyword}'."); st.stop()

        BANNED_DOMAINS = ["youtube.com", "pinterest.", "instagram.com", "facebook.com", "olx.pl", "allegro.pl", "twitter.com", "tiktok.com", "wikipedia.org", "s≈Çownik.pl", "encyklopedia.", "forum.", ".gov", ".edu", "otodom.pl", "gratka.pl", "domiporta.pl"]
        filtered_results = [r for r in top_results if r and r.get('link') and not any(b in r['link'].lower() for b in BANNED_DOMAINS)]
        if not filtered_results: st.error("Po filtracji brak artyku≈Ç√≥w do analizy."); st.stop()
        if len(top_results) > len(filtered_results): st.info(f"Pominiƒôto {len(top_results) - len(filtered_results)} wynik√≥w, analizujƒô {len(filtered_results)}.")
        # st.subheader("Analizowane adresy URL (po filtracji):") # Mo≈ºna odkomentowaƒá, je≈õli potrzebne
        # for i, r in enumerate(filtered_results, 1): st.write(f"{i}. [{r.get('title', r.get('link'))}]({r.get('link', '#')})")

        all_articles_content_str = ""
        if filtered_results:
            st.info("Etap 2/4: Pobieranie tre≈õci ze stron (ScrapingBee)...")
            all_articles_content_list, progress_bar = [], st.progress(0)
            for i, res in enumerate(filtered_results):
                if url := res.get('link'):
                    if content := scrape_and_clean_content(url, SCRAPINGBEE_API_KEY): all_articles_content_list.append(content)
                progress_bar.progress((i + 1) / len(filtered_results))
            progress_bar.empty()
            if not all_articles_content_list: st.warning("Nie uda≈Ço siƒô pobraƒá tre≈õci z ≈ºadnej strony.")
            else: st.success(f"‚úÖ Pobr. tre≈õci z {len(all_articles_content_list)} stron."); all_articles_content_str = "\n\n---\n\n".join(all_articles_content_list)
        
        if not all_articles_content_str and not filtered_results: st.error("Brak tre≈õci do analizy."); st.stop()

        st.info("Etap 3/4: Generowanie raportu przez AI (Gemini)...")
        report_parts_dict = {} # U≈ºyjemy s≈Çownika, aby ≈Çatwiej sk≈Çadaƒá raport
        report_progress = st.progress(0)
        
        sections_order = [
            "1. Kluczowe Punkty Wsp√≥lne",
            "2. Unikalne i Wyr√≥≈ºniajƒÖce Siƒô Elementy",
            "3. Sugerowane S≈Çowa Kluczowe i Semantyka",
            "4. Proponowana Struktura Artyku≈Çu (Szkic)",
            "5. Sekcja FAQ (Pytania i Odpowiedzi)"
        ]
        total_steps_for_progress = len(sections_order)

        current_step = 0
        for section_name_with_num in sections_order:
            current_step += 1
            clean_section_name = re.sub(r"^\d+\.\s*", "", section_name_with_num) # np. "Kluczowe Punkty Wsp√≥lne"
            st.write(f"Generowanie: {section_name_with_num}...")

            if section_name_with_num == "3. Sugerowane S≈Çowa Kluczowe i Semantyka":
                gemini_keywords_text = generate_s≈Çowa_kluczowe_initial_for_table(all_articles_content_str, keyword)
                st.write(f"DEBUG: Gemini (s≈Çowa kluczowe initial):\n{gemini_keywords_text if gemini_keywords_text else 'BRAK'}")

                table_data, other_text_lines, extracted_keywords_for_volume_check = [], [], []
                if gemini_keywords_text and "Brak danych" not in gemini_keywords_text and "B≈ÇƒÖd generowania" not in gemini_keywords_text:
                    kw_pattern = re.compile(r"^\s*[-*]\s+(.+)$")
                    temp_group_name = "Og√≥lne" # Domy≈õlna grupa
                    for line in gemini_keywords_text.split('\n'):
                        if line.startswith("### 3."): continue
                        if line.startswith("#### "): temp_group_name = line.replace("#### ","").strip(); other_text_lines.append(line); continue
                        
                        match = kw_pattern.match(line)
                        if match:
                            kw = match.group(1).strip()
                            if kw: extracted_keywords_for_volume_check.append(kw); table_data.append({"Grupa tematyczna": temp_group_name, "S≈Çowo kluczowe / Frazƒô": kw, "Szac. wyszuka≈Ñ/mc": "_pobieranie..._"})
                        elif line.strip(): other_text_lines.append(line.strip())
                
                section_content = f"{section_name_with_num}\n" # Zacznij od nag≈Ç√≥wka
                if table_data:
                    st.write(f"Pobieranie wolumen√≥w dla {len(extracted_keywords_for_volume_check)} fraz...")
                    keyword_volumes_map = get_keyword_volumes_dataforseo(DATAFORSEO_LOGIN, DATAFORSEO_PASSWORD, extracted_keywords_for_volume_check)
                    for row in table_data:
                        row["Szac. wyszuka≈Ñ/mc"] = keyword_volumes_map.get(row["S≈Çowo kluczowe / Frazƒô"], "brak danych") # U≈ºywamy oryginalnego s≈Çowa z tabeli
                    
                    df = pd.DataFrame(table_data)
                    section_content += df.to_markdown(index=False) + "\n\n"
                else: section_content += "Nie uda≈Ço siƒô wygenerowaƒá lub wyekstrahowaƒá s≈Ç√≥w kluczowych do tabeli.\n\n"
                if other_text_lines: section_content += "\n".join(other_text_lines)
                report_parts_dict[clean_section_name] = section_content

            elif section_name_with_num == "1. Kluczowe Punkty Wsp√≥lne":
                report_parts_dict[clean_section_name] = generate_kluczowe_punkty(all_articles_content_str, keyword)
            elif section_name_with_num == "2. Unikalne i Wyr√≥≈ºniajƒÖce Siƒô Elementy":
                report_parts_dict[clean_section_name] = generate_unikalne_elementy(all_articles_content_str, keyword)
            elif section_name_with_num == "4. Proponowana Struktura Artyku≈Çu (Szkic)":
                report_parts_dict[clean_section_name] = generate_struktura_artykulu(all_articles_content_str, keyword)
            elif section_name_with_num == "5. Sekcja FAQ (Pytania i Odpowiedzi)":
                report_parts_dict[clean_section_name] = generate_faq(all_articles_content_str, keyword)
            
            report_progress.progress(current_step / total_steps_for_progress)
            time.sleep(0.1)
        
        full_report_str = "\n\n".join(report_parts_dict.get(re.sub(r"^\d+\.\s*", "", s_name), "") for s_name in sections_order)
        report_progress.empty()

        if not full_report_str or all("Brak danych" in p or "B≈ÇƒÖd generowania" in p for p in report_parts_dict.values()):
             st.error("Generowanie raportu nie powiod≈Ço siƒô."); st.stop()

        st.info("Etap 4/4: Formatowanie wynik√≥w...")
        # Parse_report nie jest ju≈º tak kluczowy, bo mamy report_parts_dict, ale mo≈ºe siƒô przydaƒá do sp√≥jno≈õci
        report_sections_from_parse = parse_report(full_report_str) 
        
        # U≈ºyjemy report_parts_dict do wy≈õwietlania, bo mamy tam ju≈º ≈Çadnie sformatowane sekcje
        # A parse_report mo≈ºe mieƒá problemy z nowym formatowaniem sekcji 3

        st.balloons(); st.success("‚úÖ Audyt SEO gotowy!")
        st.markdown(f"--- \n## Audyt SEO i plan tre≈õci dla frazy: '{keyword}'")

        preferred_tab_order_display = ["Kluczowe Punkty Wsp√≥lne", "Unikalne i Wyr√≥≈ºniajƒÖce Siƒô Elementy", "Sugerowane S≈Çowa Kluczowe i Semantyka", "Proponowana Struktura Artyku≈Çu (Szkic)", "Sekcja FAQ (Pytania i Odpowiedzi)"]
        
        actual_tabs_to_display = []
        for clean_title_for_tab in preferred_tab_order_display:
            if clean_title_for_tab in report_parts_dict and report_parts_dict[clean_title_for_tab].strip():
                 # Usuwamy nag≈Ç√≥wek H3 z tre≈õci, bo bƒôdzie on tytu≈Çem zak≈Çadki
                content_for_tab = re.sub(rf"^\s*###\s*(?:\d+\.\s*)?{re.escape(clean_title_for_tab)}\s*\n", "", report_parts_dict[clean_title_for_tab], flags=re.IGNORECASE)
                actual_tabs_to_display.append((clean_title_for_tab, content_for_tab))
        
        if actual_tabs_to_display:
            tab_titles_only = [title for title, _ in actual_tabs_to_display]
            tabs = st.tabs(tab_titles_only)
            for i, (tab_title, tab_content) in enumerate(actual_tabs_to_display):
                with tabs[i]: 
                    st.header(tab_title) # To jest ju≈º H2 dziƒôki Streamlit Tabs
                    st.markdown(tab_content)
        else: st.warning("Brak danych do wy≈õwietlenia w zak≈Çadkach.")
else:
    if keyword: st.info(f"Wprowadzono frazƒô: '{keyword}'. Kliknij przycisk powy≈ºej, aby rozpoczƒÖƒá analizƒô.")
