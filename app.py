from urllib.parse import urlencode as encode_query_params # Dodaj ten import na g贸rze pliku, jeli go nie ma

@st.cache_data # Cache'owanie wynik贸w wyszukiwania
def get_top_10_google_results_with_scrapingbee(api_key_sb, query, num_results=10, country_code='pl', language_code='pl'):
    """Pobiera wyniki wyszukiwania Google u偶ywajc API ScrapingBee poprzez scrapowanie URL-a Google SERP."""

    sanitized_query = query.strip()
    if sanitized_query.endswith('?'):
        sanitized_query = sanitized_query[:-1].strip()

    # 1. Skonstruuj URL wyszukiwania Google
    google_search_params = {
        'q': sanitized_query,
        'hl': language_code,      # Jzyk interfejsu Google
        'gl': country_code,       # Geolokalizacja dla wynik贸w Google
        'num': str(num_results)   # Liczba wynik贸w
    }
    google_search_url = f"https://www.google.com/search?{encode_query_params(google_search_params)}"
    st.write(f"Skonstruowany URL Google Search: {google_search_url}") # Debug

    # 2. Parametry dla ScrapingBee
    params_sb = {
        'api_key': api_key_sb,
        'url': google_search_url,      # Przekazujemy peny URL Google SERP
        'custom_google': 'true',       # Informujemy ScrapingBee, 偶e to domena Google
        'premium_proxy': 'true',       # Zalecane dla Google
        'country_code': country_code,  # Lokalizacja proxy ScrapingBee (powinna pasowa do 'gl' w URL Google)
        # 'render_js': 'false'        # Zazwyczaj niepotrzebne dla SERP, mo偶e zmniejszy koszt. Mo偶na odkomentowa.
        # 'language' dla ScrapingBee mo偶e ustawia nag贸wki Accept-Language przez proxy, co te偶 mo偶e by pomocne
        # 'language': language_code, # Mo偶na rozwa偶y pozostawienie lub usunicie, skoro hl jest w URL Google
    }
    
    # Parametr 'nb_results' by dla trybu 'search', teraz liczb wynik贸w kontroluje 'num' w URL-u Google.
    # Usuwamy 'search' i 'nb_results' z params_sb, bo teraz u偶ywamy 'url'.
    # Parametr 'language' dla ScrapingBee mo偶e by przydatny do ustawienia nag贸wk贸w przez proxy.
    # Warto by byo sprawdzi, czy 'language' w ScrapingBee params i 'hl' w Google URL nie koliduj lub czy jedno nie jest wa偶niejsze.
    # Na razie zostawi 'language' zakomentowane w params_sb, bo 'hl' w URL Google jest bardziej bezporednie.


    endpoint_url = 'https://app.scrapingbee.com/api/v1/'

    try:
        st.write(f"Wysyanie zapytania do ScrapingBee z parametrami: {params_sb}") # Debug
        response = requests.get(endpoint_url, params=params_sb, timeout=90) # Zwikszony timeout
        response.raise_for_status()
        data = response.json()
        st.write(f"Odpowied藕 JSON od ScrapingBee: {data}") # Debug

        # Struktura odpowiedzi ScrapingBee przy scrapowaniu Google SERP mo偶e by inna
        # ni偶 przy ich dedykowanym parametrze 'search' (jeli taki tryb istnieje i dziaa inaczej).
        # Trzeba bdzie sprawdzi, czy 'organic_results' nadal jest poprawnym kluczem.
        if 'organic_results' in data and data['organic_results']:
            results = []
            for item in data['organic_results']:
                title = item.get('title')
                link = item.get('link')
                if title and link:
                    results.append({'title': title, 'link': link})
                else:
                    st.warning(f"Pominito wynik z ScrapingBee z powodu braku tytuu lub linku: {item}")
            return results
        elif 'error' in data: # Sprawd藕, czy ScrapingBee nie zwr贸cio wasnego bdu w JSON
             st.warning(f"ScrapingBee zwr贸cio bd w odpowiedzi JSON: {data.get('error_message', data['error'])}")
             return []
        else: # Jeli nie ma 'organic_results' ani 'error', wywietl ostrze偶enie.
            st.warning(f"ScrapingBee nie zwr贸cio 'organic_results' dla zapytania (URL: {google_search_url}). Sprawd藕 odpowied藕 JSON powy偶ej.")
            return []

    except requests.exceptions.Timeout:
        st.error(f" Przekroczono czas oczekiwania na odpowied藕 od ScrapingBee dla URL: '{google_search_url}'")
        return None
    except requests.exceptions.RequestException as e:
        safe_params_for_log = params_sb.copy()
        safe_params_for_log['api_key'] = "REDACTED_API_KEY"
        # URL jest ju偶 w params_sb, wic nie trzeba go dodatkowo kodowa do log贸w
        st.error(f" Bd podczas komunikacji z API ScrapingBee: {e}. Parametry wysane (z zredagowanym kluczem): {safe_params_for_log}")
        return None
    except Exception as e:
        st.error(f" Nieoczekiwany bd podczas przetwarzania odpowiedzi z ScrapingBee (np. bd JSON): {e}")
        if 'response' in locals() and hasattr(response, 'text'):
            st.text_area("Surowa odpowied藕 (debug):", response.text, height=150)
        return None
