# app.py - Wersja PRO z kompletnym audytem SEO i zakadkami

import streamlit as st
import requests
import re
from trafilatura import extract
import google.generativeai as genai
from googleapiclient.discovery import build
# Usunito import ThreadPoolExecutor, bo wracamy do sekwencyjnego scrapingu

# --- Konfiguracja strony ---
st.set_page_config(page_title="SEO Content Powerhouse", page_icon="", layout="wide")
st.title(" SEO Content Powerhouse z AI")
st.markdown("Narzdzie do tworzenia kompletnych strategii contentowych na podstawie analizy TOP 10 wynik贸w Google.")

# --- Obsuga Kluczy API ---
try:
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
    SEARCH_API_KEY = st.secrets["SEARCH_API_KEY"]
    SEARCH_ENGINE_ID = st.secrets["SEARCH_ENGINE_ID"]
    # Zmieniono nazw klucza na SCRAPE_DO_API_KEY
    SCRAPE_DO_API_KEY = st.secrets["SCRAPE_DO_API_KEY"] 
    genai.configure(api_key=GEMINI_API_KEY)
except (KeyError, FileNotFoundError):
    # Zaktualizowany komunikat bdu dla SCRAPE_DO_API_KEY
    st.error("Bd: Klucze API nie zostay znalezione. Upewnij si, 偶e skonfigurowae WSZYSTKIE 4 sekrety w Streamlit (w tym SCRAPE_DO_API_KEY).")
    st.stop()

# --- Funkcje Backendowe ---
@st.cache_data
def get_top_10_results(api_key, cse_id, query):
    service = build("customsearch", "v1", developerKey=api_key)
    res = service.cse().list(q=query, cx=cse_id, num=10, gl='pl', hl='pl').execute()
    return res.get('items', [])

@st.cache_data # Cache jest zachowany, bo scraping wraca do sekwencyjnego
def scrape_and_clean_content(url_to_scrape): # Usunito api_key_for_scrape_do jako argument
    try:
        # Zmieniono API z ScrapingBee na scrape.do
        # U偶ywamy SCRAPE_DO_API_KEY bezporednio z zakresu globalnego
        response = requests.get(
            url=f'https://api.scrape.do/?token={SCRAPE_DO_API_KEY}&url={url_to_scrape}',
            timeout=60
        )
        response.raise_for_status()
        # Wr贸cono do domylnego wyjcia trafilatura (plain text)
        return extract(response.text, include_comments=False, include_tables=False) 
    except requests.exceptions.RequestException as e:
        st.warning(f"Nie udao si pobra treci z {url_to_scrape} przy u偶yciu scrape.do: {e}")
        return None

def analyze_content_with_gemini(all_content, keyword_phrase):
    if not all_content: return "Brak treci do analizy."
    
    # --- PRZYWRCONY ORYGINALNY PROMPT Z POCZTKU ---
    prompt = f"""
    Jeste wiatowej klasy analitykiem SEO i strategiem content marketingu. Przeanalizuj zagregowan tre z czoowych artyku贸w dla frazy "{keyword_phrase}" i na tej podstawie wygeneruj kompleksowy raport w formacie Markdown. Raport musi by podzielony na DOKADNIE nastpujce sekcje, u偶ywajc nag贸wk贸w `### numer. Nazwa sekcji`:

    ### 1. Kluczowe Punkty Wsp贸lne
    (Wypunktuj tematy, kt贸re powtarzaj si w wikszoci tekst贸w.)

    ### 2. Unikalne i Wyr贸偶niajce Si Elementy
    (Wypunktuj ciekawe informacje, kt贸re pojawiy si tylko w niekt贸rych 藕r贸dach.)

    ### 3. Sugerowane Sowa Kluczowe i Semantyka
    (Stw贸rz list 15-20 najwa偶niejszych s贸w kluczowych i fraz powizanych. Pogrupuj je tematycznie, jeli to ma sens.)

    ### 4. Proponowana Struktura Artykuu (Szkic)
    (Zaproponuj idealn struktur nowego artykuu w formie nag贸wk贸w H2 i H3, od wstpu po podsumowanie.)

    ### 5. Sekcja FAQ (Pytania i Odpowiedzi)
    (Stw贸rz list 4-5 najwa偶niejszych pyta w stylu 'People Also Ask' i udziel na nie zwizych odpowiedzi.)

    ### 6. Wnioski i Rekomendacje
    (Stw贸rz list praktycznych porad dla osoby, kt贸ra chce napisa najlepszy artyku na ten temat.)
    """
    
    model = genai.GenerativeModel('gemini-1.5-flash-latest')
    # Usunito timeout, bo nie byo go w oryginalnej wersji promptu
    response = model.generate_content(prompt) 
    return response.text

# --- NOWA FUNKCJA DO PARSOWANIA RAPORTU ---
def parse_report(report_text):
    """Dzieli peny raport na sekcje do wywietlenia w zakadkach."""
    sections = {}
    pattern = r"###\s*\d+\.\s*(.*?)\n(.*?)(?=\n###\s*\d+\.|$)"
    matches = re.findall(pattern, report_text, re.DOTALL)
    
    for match in matches:
        title = match[0].strip()
        content = match[1].strip()
        sections[title] = content
        
    return sections

# --- Interfejs U偶ytkownika ---
keyword = st.text_input("Wprowad藕 fraz kluczow, kt贸r chcesz przeanalizowa:", placeholder="np. jak dba o buty sk贸rzane")

if st.button(" Wygeneruj Kompleksowy Audyt SEO"):
    if keyword:
        with st.spinner("Przeprowadzam peny audyt... To mo偶e potrwa kilka minut."):
            st.write("Etap 1/4: Pobieranie i filtrowanie wynik贸w z Google...")
            top_results = get_top_10_results(SEARCH_API_KEY, SEARCH_ENGINE_ID, keyword)
            if not top_results: st.error("Nie znaleziono wynik贸w."); st.stop()
            
            BANNED_DOMAINS = ["youtube.com", "pinterest.", "instagram.com", "facebook.com", "olx.pl", "allegro.pl"]
            filtered_results = [r for r in top_results if not any(b in r.get('link','') for b in BANNED_DOMAINS)]
            
            if not filtered_results: st.error("Po filtracji nie pozostay 偶adne artykuy do analizy."); st.stop()
            st.info(f"Pominito {len(top_results) - len(filtered_results)} wynik贸w (wideo/social media), analizuj {len(filtered_results)} artyku贸w.")

            st.write("Etap 2/4: Pobieranie treci ze stron przez scrape.do API...") # Zmieniony tekst komunikatu
            all_articles_content, successful_sources = [], []
            progress_bar = st.progress(0)
            
            # Wr贸cono do sekwencyjnego pobierania, usunito ThreadPoolExecutor
            for i, result in enumerate(filtered_results):
                # scrape_and_clean_content nie przyjmuje ju偶 api_key jako argumentu
                content = scrape_and_clean_content(result.get('link')) 
                if content:
                    all_articles_content.append(content)
                    successful_sources.append({'title': result.get('title'), 'link': result.get('link')})
                progress_bar.progress((i + 1) / len(filtered_results))

            if not all_articles_content: st.error("Nie udao si pobra treci z 偶adnej ze stron."); st.stop()

            st.write("Etap 3/4: Generowanie kompleksowego raportu przez AI...")
            aggregated_content = "\n\n---\n\n".join(all_articles_content)
            
            # Sprawdzenie czy aggregated_content nie jest pusty
            if not aggregated_content.strip():
                st.error("Nie pozostaa 偶adna tre do analizy przez AI (po agregacji).")
                st.stop()

            full_report = analyze_content_with_gemini(aggregated_content, keyword)
            
            st.write("Etap 4/4: Formatowanie wynik贸w...")
            report_sections = parse_report(full_report)
            
            st.balloons()
            st.success("Audyt SEO gotowy!")
            
            st.markdown(f"--- \n## Audyt SEO i plan treci dla frazy: '{keyword}'")
            
            # --- INTERFEJS Z ZAKADKAMI (jak w oryginalnym kodzie) ---
            tab_titles = [
                "Punkty Wsp贸lne", "Unikalne Elementy", "Sowa Kluczowe",
                "Struktura Artykuu", "FAQ", "Rekomendacje"
            ]
            
            tabs = st.tabs([f" {title}" for title in tab_titles])

            with tabs[0]:
                st.markdown(report_sections.get("Kluczowe Punkty Wsp贸lne", "Brak danych."))
            with tabs[1]:
                st.markdown(report_sections.get("Unikalne i Wyr贸偶niajce Si Elementy", "Brak danych."))
            with tabs[2]:
                st.markdown(report_sections.get("Sugerowane Sowa Kluczowe i Semantyka", "Brak danych."))
            with tabs[3]:
                st.markdown(report_sections.get("Proponowana Struktura Artykuu (Szkic)", "Brak danych."))
            with tabs[4]:
                st.markdown(report_sections.get("Sekcja FAQ (Pytania i Odpowiedzi)", "Brak danych."))
            with tabs[5]:
                st.markdown(report_sections.get("Wnioski i Rekomendacje", "Brak danych."))

            # Rozwijana lista ze 藕r贸dami na kocu
            with st.expander(f"Zobacz {len(successful_sources)} 藕r贸de, kt贸re zostay pomylnie przeanalizowane"):
                if successful_sources:
                    for source in successful_sources:
                        st.markdown(f"- **{source['title']}**\n  - [{source['link']}]({source['link']})")
                else:
                    st.markdown("Brak 藕r贸de, z kt贸rych udao si pobra tre.")
    else:
        st.warning("Prosz wpisa fraz kluczow.")
