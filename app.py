# -*- coding: utf-8 -*-

# ==============================================================================
# Krok 1: Import bibliotek
# ==============================================================================
import streamlit as st
import requests
import re
import time # <<< ZMIANA KRYTYCZNA: Importujemy bibliotekÄ™ time >>>
from trafilatura import extract
import google.generativeai as genai
from googleapiclient.discovery import build

# ==============================================================================
# Krok 2: Konfiguracja strony Streamlit
# ==============================================================================
st.set_page_config(page_title="SEO Content Powerhouse", page_icon="ğŸš€", layout="wide")
st.title("ğŸš€ SEO Content Powerhouse z AI")
st.markdown("NarzÄ™dzie do tworzenia kompletnych strategii contentowych na podstawie analizy TOP 10 wynikÃ³w Google.")

# ==============================================================================
# Krok 3: ObsÅ‚uga Kluczy API ze Streamlit Secrets
# ==============================================================================
try:
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
    SEARCH_API_KEY = st.secrets["SEARCH_API_KEY"]
    SEARCH_ENGINE_ID = st.secrets["SEARCH_ENGINE_ID"]
    SCRAPINGBEE_API_KEY = st.secrets["SCRAPINGBEE_API_KEY"]
    genai.configure(api_key=GEMINI_API_KEY)
except KeyError as e:
    st.error(f"ğŸ›‘ BÅ‚Ä…d konfiguracji sekretÃ³w! Nie znaleziono wymaganego sekretu: {e}.")
    st.stop()

# ==============================================================================
# Krok 4: Funkcje Backendowe
# ==============================================================================

# <<< ZMIANA KRYTYCZNA: Dodajemy dodatkowy argument `_` do funkcji >>>
# DziÄ™ki temu moÅ¼emy przekazaÄ‡ unikalnÄ… wartoÅ›Ä‡ (np. czas), aby za kaÅ¼dym razem ominÄ…Ä‡ cache.
@st.cache_data
def get_top_10_results(api_key, cse_id, query, _=None):
    """Pobiera 10 najlepszych wynikÃ³w wyszukiwania Google dla danej frazy."""
    try:
        service = build("customsearch", "v1", developerKey=api_key)
        res = service.cse().list(q=query, cx=cse_id, num=10, gl='pl', hl='pl').execute()
        if 'items' not in res or not res['items']:
            return []
        return [{'title': item.get('title'), 'link': item.get('link')} for item in res['items']]
    except Exception as e:
        st.error(f"ğŸ›‘ BÅ‚Ä…d podczas pobierania wynikÃ³w z Google Search API: {e}")
        return None

@st.cache_data
def scrape_and_clean_content(url_to_scrape, scrapingbee_api_key):
    """Pobiera i czyÅ›ci treÅ›Ä‡ ze strony uÅ¼ywajÄ…c ScrapingBee."""
    try:
        response = requests.get(
            url='https://app.scrapingbee.com/api/v1/',
            params={'api_key': scrapingbee_api_key, 'url': url_to_scrape, 'premium_proxy': 'true', 'block_resources': 'false'},
            timeout=90
        )
        response.raise_for_status()
        extracted_text = extract(response.text, include_comments=False, include_tables=False, include_images=False)
        if not extracted_text:
             return None
        cleaned_text = re.sub(r'\s+', ' ', extracted_text).strip()
        return cleaned_text if len(cleaned_text) > 100 else None
    except Exception as e:
        st.warning(f"âš ï¸ Nie udaÅ‚o siÄ™ pobraÄ‡ treÅ›ci z {url_to_scrape}: {e}")
        return None

@st.cache_data(show_spinner="AI analizuje treÅ›Ä‡...")
def analyze_content_with_gemini(all_content, keyword_phrase):
    """Analizuje zagregowanÄ… treÅ›Ä‡ i generuje raport z Gemini."""
    if not all_content:
        return "Brak treÅ›ci do analizy przez AI."

    # Ulepszony prompt
    prompt = f"""
JesteÅ› Å›wiatowej klasy analitykiem SEO. Przeanalizuj treÅ›Ä‡ z czoÅ‚owych artykuÅ‚Ã³w dla frazy "{keyword_phrase}" i wygeneruj kompleksowy raport w Markdown.

Twoja odpowiedÅº MUSI byÄ‡ podzielona na DOKÅADNIE 5 sekcji, uÅ¼ywajÄ…c nagÅ‚Ã³wkÃ³w w formacie `### [Numer]. [Nazwa Sekcji]`.

### 1. Kluczowe Punkty WspÃ³lne
Wypunktuj tematy i informacje, ktÃ³re powtarzajÄ… siÄ™ w wiÄ™kszoÅ›ci tekstÃ³w. To jest standard w TOP 10.

### 2. Unikalne i WyrÃ³Å¼niajÄ…ce SiÄ™ Elementy
Wypunktuj oryginalne informacje, dane, przykÅ‚ady, ktÃ³re mogÄ… daÄ‡ przewagÄ™ konkurencyjnÄ….

### 3. Sugerowane SÅ‚owa Kluczowe i Semantyka
StwÃ³rz listÄ™ 10-15 sÅ‚Ã³w kluczowych i fraz. OkreÅ›l intencjÄ™ wyszukiwania.

### 4. Proponowana Struktura ArtykuÅ‚u (Szkic)
StwÃ³rz ROZBUDOWANY plan artykuÅ‚u. Twoja odpowiedÅº dla tej sekcji MUSI zawieraÄ‡:
1.  Jeden chwytliwy tytuÅ‚ (jako `# TytuÅ‚ ArtykuÅ‚u`).
2.  KrÃ³tki wstÄ™p (2-3 zdania).
3.  ListÄ™ co najmniej 4-5 gÅ‚Ã³wnych sekcji (jako `## NagÅ‚Ã³wek Sekcji`).
4.  Dla kaÅ¼dej sekcji H2, zaproponuj 2-4 podpunkty (jako `### Podpunkt`).

### 5. Sekcja FAQ (Pytania i Odpowiedzi)
StwÃ³rz 4-5 pytaÅ„ w stylu 'People Also Ask' z krÃ³tkimi odpowiedziami.

PamiÄ™taj, Twoja odpowiedÅº to TYLKO treÅ›Ä‡ raportu w Markdown, po polsku.
TreÅ›Ä‡ do analizy:
{all_content}
"""
    try:
        model = genai.GenerativeModel('gemini-1.5-flash-latest')
        response = model.generate_content(prompt)
        return response.text if hasattr(response, 'text') else None
    except Exception as e:
        st.error(f"ğŸ›‘ BÅ‚Ä…d podczas komunikacji z Gemini API: {e}")
        return None

# Ulepszony parser
def parse_report(report_text):
    if not report_text: return {}
    sections = {}
    pattern = r"###\s*(\d+\.\s*.*?)\n(.*?)(?=\n###\s*\d+\.|$)"
    matches = re.findall(pattern, report_text, re.DOTALL | re.MULTILINE)
    for match in matches:
        title = match[0].strip()
        content = match[1].strip()
        if title:
            clean_title = re.sub(r"^\d+\.\s*", "", title)
            sections[clean_title] = content
    return sections

# ==============================================================================
# Krok 5: Interfejs UÅ¼ytkownika Streamlit i gÅ‚Ã³wna logika
# ==============================================================================
keyword = st.text_input("WprowadÅº frazÄ™ kluczowÄ…, ktÃ³rÄ… chcesz przeanalizowaÄ‡:", placeholder="np. jak dbaÄ‡ o buty skÃ³rzane")

if st.button("ğŸš€ Wygeneruj Kompleksowy Audyt SEO"):
    if not keyword:
        st.warning("ProszÄ™ wpisaÄ‡ frazÄ™ kluczowÄ….")
        st.stop()

    with st.spinner("Przeprowadzam peÅ‚ny audyt... To moÅ¼e potrwaÄ‡ kilka minut."):
        st.info("Etap 1/4: Pobieranie i filtrowanie wynikÃ³w z Google...")
        
        # <<< ZMIANA KRYTYCZNA: Przekazujemy `time.time()` jako dodatkowy argument >>>
        # To powoduje, Å¼e Streamlit traktuje to wywoÅ‚anie jako unikalne i nie uÅ¼ywa starego cache'u.
        top_results = get_top_10_results(SEARCH_API_KEY, SEARCH_ENGINE_ID, keyword, _=time.time())

        if top_results is None:
            st.error("WystÄ…piÅ‚ bÅ‚Ä…d podczas pobierania danych z Google. SprawdÅº konfiguracjÄ™ API.")
            st.stop()
        if not top_results:
            st.error(f"Nie znaleziono Å¼adnych wynikÃ³w TOP 10 dla frazy: '{keyword}'. SprawdÅº konfiguracjÄ™ SEARCH_ENGINE_ID w panelu Google - czy na pewno ma wÅ‚Ä…czone 'Search the entire web'?")
            st.stop()

        BANNED_DOMAINS = ["youtube.com", "pinterest.", "instagram.com", "facebook.com", "olx.pl", "allegro.pl", "twitter.com", "tiktok.com", "wikipedia.org", ".gov", ".edu", "ceneo.pl", "skapiec.pl"]
        filtered_results = [r for r in top_results if r and r.get('link') and not any(b in r['link'].lower() for b in BANNED_DOMAINS)]

        if not filtered_results:
            st.error("Po filtracji nie pozostaÅ‚y Å¼adne artykuÅ‚y do analizy.")
            st.stop()

        if len(top_results) > len(filtered_results):
            st.info(f"PominiÄ™to {len(top_results) - len(filtered_results)} wynikÃ³w, analizujÄ™ {len(filtered_results)} znalezionych artykuÅ‚Ã³w.")
        
        with st.expander("Zobacz analizowane adresy URL"):
            for i, result in enumerate(filtered_results, 1):
                st.write(f"{i}. [{result.get('title', 'Brak tytuÅ‚u')}]({result.get('link', '#')})")

        st.info("Etap 2/4: Pobieranie treÅ›ci ze stron...")
        all_articles_content, successful_sources = [], []
        progress_bar = st.progress(0, text="Pobieranie...")
        for i, result in enumerate(filtered_results):
             url = result.get('link')
             if url:
                 content = scrape_and_clean_content(url, SCRAPINGBEE_API_KEY)
                 if content:
                     all_articles_content.append(content)
                     successful_sources.append({'title': result.get('title', 'Brak tytuÅ‚u'), 'link': url})
                 progress_bar.progress((i + 1) / len(filtered_results), text=f"Pobrano {i+1}/{len(filtered_results)}")
        progress_bar.empty()

        if not all_articles_content:
            st.error("Nie udaÅ‚o siÄ™ pobraÄ‡ treÅ›ci z Å¼adnej ze stron.")
            st.stop()
        st.success(f"âœ… PomyÅ›lnie pobrano treÅ›ci z {len(all_articles_content)} stron.")

        st.info("Etap 3/4: Generowanie kompleksowego raportu przez AI...")
        aggregated_content = "\n\n---\n\n".join(all_articles_content)
        full_report = analyze_content_with_gemini(aggregated_content, keyword)

        if not full_report:
             st.error("Generowanie raportu przez Gemini nie powiodÅ‚o siÄ™.")
             st.stop()

        st.info("Etap 4/4: Formatowanie wynikÃ³w...")
        report_sections = parse_report(full_report)
        sources_text = "\n".join([f"- [{source['title']}]({source['link']})" for source in successful_sources])
        report_sections["Analizowane Å¹rÃ³dÅ‚a"] = "PoniÅ¼ej lista adresÃ³w URL, ktÃ³rych treÅ›Ä‡ zostaÅ‚a pomyÅ›lnie pobrana i przeanalizowana przez AI:\n" + sources_text

        st.balloons()
        st.success("âœ… Audyt SEO gotowy!")
        st.markdown(f"--- \n## Audyt SEO i plan treÅ›ci dla frazy: '{keyword}'")

        preferred_tab_order = ["Kluczowe Punkty WspÃ³lne", "Unikalne i WyrÃ³Å¼niajÄ…ce SiÄ™ Elementy", "Sugerowane SÅ‚owa Kluczowe i Semantyka", "Proponowana Struktura ArtykuÅ‚u (Szkic)", "Sekcja FAQ (Pytania i Odpowiedzi)", "Analizowane Å¹rÃ³dÅ‚a"]
        actual_tab_titles = [title for title in preferred_tab_order if title in report_sections and report_sections[title].strip()]

        if actual_tab_titles:
             tabs = st.tabs(actual_tab_titles)
             for i, tab in enumerate(tabs):
                 with tab:
                     current_title = actual_tab_titles[i]
                     st.header(current_title)
                     st.markdown(report_sections[current_title], unsafe_allow_html=True)
        else:
             st.warning("Brak danych do wyÅ›wietlenia. OdpowiedÅº z AI mogÅ‚a byÄ‡ pusta lub w nieprawidÅ‚owym formacie.")
else:
    if keyword:
         st.info(f"Wprowadzono frazÄ™: '{keyword}'. Kliknij przycisk powyÅ¼ej, aby rozpoczÄ…Ä‡ analizÄ™.")
