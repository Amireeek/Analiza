# app.py - wersja z profesjonalnym Scraping API (ScrapingBee)

import streamlit as st
import requests  # Wracamy do requests, bo jest prostsze do tego API
from trafilatura import extract
import google.generativeai as genai
from googleapiclient.discovery import build

# --- Konfiguracja strony (bez zmian) ---
st.set_page_config(page_title="Analizator SERP z Gemini", page_icon="", layout="wide")
st.title(" Analizator SERP z AI")
st.markdown("Narzdzie do gbokiej analizy treci z TOP 10 wynik贸w Google przy u偶yciu Gemini 1.5 Pro.")

# --- Obsuga Kluczy API (Z DODATKOWYM KLUCZEM) ---
try:
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
    SEARCH_API_KEY = st.secrets["SEARCH_API_KEY"]
    SEARCH_ENGINE_ID = st.secrets["SEARCH_ENGINE_ID"]
    # NOWY KLUCZ
    SCRAPINGBEE_API_KEY = st.secrets["SCRAPINGBEE_API_KEY"]
    
    genai.configure(api_key=GEMINI_API_KEY)
except (KeyError, FileNotFoundError):
    st.error("Bd: Klucze API nie zostay znalezione. Upewnij si, 偶e skonfigurowae WSZYSTKIE 4 sekrety w Streamlit (w tym SCRAPINGBEE_API_KEY).")
    st.stop()

# --- Funkcje Backendowe (Z AKTUALIZACJ OSTATECZN) ---
@st.cache_data
def get_top_10_results(api_key, cse_id, query):
    service = build("customsearch", "v1", developerKey=api_key)
    res = service.cse().list(q=query, cx=cse_id, num=10, gl='pl', hl='pl').execute()
    return res.get('items', [])

# --- OSTATECZNA WERSJA FUNKCJI SCRAPINGU Z U呕YCIEM SCRAPINGBEE ---
@st.cache_data
def scrape_and_clean_content(url_to_scrape):
    try:
        # Budujemy zapytanie do API ScrapingBee
        response = requests.get(
            url='https://app.scrapingbee.com/api/v1/',
            params={
                'api_key': SCRAPINGBEE_API_KEY,
                'url': url_to_scrape,
                'render_js': 'false',  # Ustaw na 'true' jeli strona wymaga JS, ale zu偶ywa wicej kredyt贸w
                'premium_proxy': 'true', # U偶ywa lepszych, rezydencjalnych proxy
            },
            timeout=60  # Dajemy wicej czasu, bo proces jest bardziej zo偶ony
        )
        # Sprawdzamy, czy samo API ScrapingBee nie zwr贸cio bdu
        response.raise_for_status()
        
        # Jeli wszystko jest OK, przekazujemy zwr贸con tre HTML do trafilatura
        return extract(response.text, include_comments=False, include_tables=False)
        
    except requests.exceptions.RequestException as e:
        st.warning(f"Nie udao si pobra treci z {url_to_scrape} przez ScrapingBee: {e}")
        return None
    except Exception as e:
        st.warning(f"Wystpi inny bd podczas przetwarzania {url_to_scrape}: {e}")
        return None

# --- Pozostae funkcje (bez zmian) ---
def analyze_content_with_gemini(all_content, keyword_phrase):
    if not all_content: return "Brak treci do analizy."
    
    prompt = f"""
    Jeste ekspertem SEO i analitykiem content marketingu. Przeanalizuj zagregowan tre z czoowych artyku贸w dla frazy "{keyword_phrase}" i na tej podstawie:
    1.  **Zidentyfikuj kluczowe punkty wsp贸lne:** Wypunktuj tematy, kt贸re powtarzaj si w wikszoci tekst贸w.
    2.  **Wska偶 unikalne elementy:** Wypunktuj ciekawe informacje, kt贸re pojawiy si tylko w niekt贸rych 藕r贸dach.
    3.  **Sformuuj wnioski i rekomendacje:** Stw贸rz list praktycznych porad dla kogo, kto chce napisa najlepszy artyku na ten temat.
    Sformatuj odpowied藕 u偶ywajc czytelnego Markdown.
    """
    
    model = genai.GenerativeModel('gemini-1.5-flash-latest')
    response = model.generate_content(prompt)
    return response.text

# --- Interfejs U偶ytkownika (bez zmian) ---
keyword = st.text_input("Wprowad藕 fraz kluczow, kt贸r chcesz przeanalizowa:", placeholder="np. jak czy ubrania w zestawy")

if st.button(" Rozpocznij Analiz"):
    if keyword:
        with st.spinner("Trwa analiza... U偶ywamy zaawansowanych technik, to mo偶e potrwa kilka minut."):
            st.write("Krok 1/3: Pobieranie listy TOP 10 wynik贸w z Google...")
            top_results = get_top_10_results(SEARCH_API_KEY, SEARCH_ENGINE_ID, keyword)
            if not top_results: st.error("Nie znaleziono wynik贸w dla tej frazy."); st.stop()

            st.write("Krok 2/3: Pobieranie treci przez Scraping API (omijanie zabezpiecze)...")
            all_articles_content = []
            progress_bar = st.progress(0)
            for i, result in enumerate(top_results):
                url = result.get('link')
                content = scrape_and_clean_content(url)
                if content: all_articles_content.append(content)
                progress_bar.progress((i + 1) / len(top_results))

            if not all_articles_content: st.error("Nie udao si pobra treci z 偶adnej ze stron, nawet przy u偶yciu zaawansowanych technik."); st.stop()

            st.write("Krok 3/3: Przekazywanie treci do analizy przez AI...")
            aggregated_content = "\n\n---\n\n".join(all_articles_content)
            analysis_report = analyze_content_with_gemini(aggregated_content, keyword)
            
            st.balloons()
            st.success("Analiza zakoczona!")
            st.markdown(f"--- \n## 娣卞害 Pena Analiza SERP dla frazy: '{keyword}'")
            st.markdown(analysis_report)
    else:
        st.warning("Prosz wpisa fraz kluczow.")
