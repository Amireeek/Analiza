# -*- coding: utf-8 -*-

# ==============================================================================
# Krok 0: Instalacja bibliotek
# ==============================================================================
# Je≈õli uruchamiasz to lokalnie, upewnij siƒô, ≈ºe masz te biblioteki zainstalowane:
# pip install streamlit requests trafilatura google-generativeai google-api-python-client scrapingbee
# Je≈õli uruchamiasz w Streamlit Cloud, dodaj je do pliku requirements.txt

# ==============================================================================
# Krok 1: Import bibliotek
# ==============================================================================
import streamlit as st
import requests
import re
from trafilatura import extract # Upewnij siƒô, ≈ºe masz tƒô bibliotekƒô
import google.generativeai as genai
from googleapiclient.discovery import build # Upewnij siƒô, ≈ºe masz tƒô bibliotekƒô


# ==============================================================================
# Krok 2: Konfiguracja strony Streamlit
# ==============================================================================
st.set_page_config(page_title="SEO Content Powerhouse", page_icon="üöÄ", layout="wide")
st.title("üöÄ SEO Content Powerhouse z AI")
st.markdown("Narzƒôdzie do tworzenia kompletnych strategii contentowych na podstawie analizy TOP 10 wynik√≥w Google.")


# ==============================================================================
# Krok 3: Obs≈Çuga Kluczy API ze Streamlit Secrets
# ==============================================================================
# WA≈ªNE: Upewnij siƒô, ≈ºe skonfigurowa≈Çe≈õ WSZYSTKIE 4 klucze jako sekrety w Streamlit
try:
    # Rƒôcznie wpisz te linie, aby uniknƒÖƒá problem√≥w z niewidocznymi znakami!
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
    SEARCH_API_KEY = st.secrets["SEARCH_API_KEY"]
    SEARCH_ENGINE_ID = st.secrets["SEARCH_ENGINE_ID"]
    SCRAPINGBEE_API_KEY = st.secrets["SCRAPINGBEE_API_KEY"] # Klucz ScrapingBee

    genai.configure(api_key=GEMINI_API_KEY)
    # Nie konfigurujemy od razu Google Search API, bo 'build' jest u≈ºywane w funkcji

    #st.success("‚úÖ Klucze API za≈Çadowane pomy≈õlnie.") # Mo≈ºna odkomentowaƒá dla debugowania

except KeyError as e:
    st.error(f"üõë B≈ÇƒÖd konfiguracji sekret√≥w! Nie znaleziono wymaganego sekretu: {e}. Upewnij siƒô, ≈ºe skonfigurowa≈Çe≈õ WSZYSTKIE 4 sekrety (GEMINI_API_KEY, SEARCH_API_KEY, SEARCH_ENGINE_ID, SCRAPINGBEE_API_KEY) w ustawieniach Streamlit.")
    st.stop() # Zatrzymaj dzia≈Çanie aplikacji, je≈õli klucze nie sƒÖ skonfigurowane
except Exception as e:
    st.error(f"üõë WystƒÖpi≈Ç nieoczekiwany b≈ÇƒÖd podczas ≈Çadowania kluczy: {e}")
    st.stop()


# ==============================================================================
# Krok 4: Funkcje Backendowe
# ==============================================================================

@st.cache_data # Cache'owanie wynik√≥w wyszukiwania Google
def get_top_10_results(api_key, cse_id, query):
    """Pobiera 10 najlepszych wynik√≥w wyszukiwania Google dla danej frazy."""
    try:
        # U≈ºywamy wersji developera, kt√≥ra wymaga klucza API
        service = build("customsearch", "v1", developerKey=api_key)
        # num=10 ogranicza wyniki do 10, gl/hl=pl ustawia region i jƒôzyk na polski
        res = service.cse().list(q=query, cx=cse_id, num=10, gl='pl', hl='pl').execute()

        if 'items' not in res or not res['items']:
            # st.warning("Nie znaleziono ≈ºadnych wynik√≥w dla tej frazy.") # Komunikat bƒôdzie ni≈ºej w UI
            return []

        # Zwracamy listƒô s≈Çownik√≥w z tytu≈Çem i linkiem
        return [{'title': item.get('title'), 'link': item.get('link')} for item in res['items']]
    except Exception as e:
        st.error(f"üõë B≈ÇƒÖd podczas pobierania wynik√≥w z Google Search API: {e}")
        # st.info("Upewnij siƒô, ≈ºe Tw√≥j SEARCH_API_KEY jest poprawny i w≈ÇƒÖczy≈Çe≈õ Custom Search API w Google Cloud.") # Mo≈ºna dodaƒá wiƒôcej wskaz√≥wek
        return None # Zwr√≥ƒá None w przypadku b≈Çƒôdu, aby go obs≈Çu≈ºyƒá dalej


@st.cache_data # Cache'owanie pobranej tre≈õci
def scrape_and_clean_content(url_to_scrape, scrapingbee_api_key):
    """Pobiera i czy≈õci tre≈õƒá ze strony u≈ºywajƒÖc ScrapingBee."""
    try:
        response = requests.get(
            url='https://app.scrapingbee.com/api/v1/',
            params={'api_key': scrapingbee_api_key, 'url': url_to_scrape, 'premium_proxy': 'true', 'block_resources': 'false'}, # Dodano block_resources
            timeout=90 # Zwiƒôkszono timeout na wypadek wolnych stron
        )
        response.raise_for_status() # Rzuca wyjƒÖtek dla kod√≥w b≈Çƒôd√≥w HTTP (4xx, 5xx)

        # U≈ºywamy trafilatury do ekstrakcji czystego tekstu artyku≈Çu
        # Upewnij siƒô, ≈ºe tre≈õƒá response.text jest odpowiednia (np. zakodowana w UTF-8)
        extracted_text = extract(response.text, include_comments=False, include_tables=False, include_images=False) # Wy≈ÇƒÖczono obrazy

        if not extracted_text:
             #st.warning(f"Trafilatura nie zwr√≥ci≈Ça tre≈õci dla {url_to_scrape}") # Debug
             return None

        # Opcjonalnie: dodatkowe czyszczenie tekstu (np. usuniƒôcie nadmiernych bia≈Çych znak√≥w)
        cleaned_text = re.sub(r'\s+', ' ', extracted_text).strip()

        return cleaned_text if len(cleaned_text) > 100 else None # Zwr√≥ƒá None je≈õli tre≈õƒá jest za kr√≥tka

    except requests.exceptions.RequestException as e:
        st.warning(f"‚ö†Ô∏è Nie uda≈Ço siƒô pobraƒá tre≈õci z {url_to_scrape} (ScrapingBee): {e}")
        return None
    except Exception as e:
        st.warning(f"‚ö†Ô∏è WystƒÖpi≈Ç nieoczekiwany b≈ÇƒÖd podczas przetwarzania tre≈õci z {url_to_scrape}: {e}")
        return None


@st.cache_data(show_spinner="AI analizuje tre≈õƒá...") # Cache'owanie wynik√≥w Gemini z innym spinnerem
def analyze_content_with_gemini(all_content, keyword_phrase):
    """Analizuje zagregowanƒÖ tre≈õƒá i generuje raport z Gemini."""
    if not all_content:
        return "Brak tre≈õci do analizy przez AI."

    # === ZMIANA: ZMODYFIKOWANY PROMPT DLA LEPSZEJ STRUKTURY ARTYKU≈ÅU ===
    prompt = f"""
Jeste≈õ ≈õwiatowej klasy analitykiem SEO i strategiem content marketingu. Twoim zadaniem jest przeanalizowanie dostarczonej tre≈õci z czo≈Çowych artyku≈Ç√≥w dla frazy "{keyword_phrase}" i na tej podstawie wygenerowanie kompleksowego raportu w formacie Markdown.

Raport musi byƒá podzielony na DOK≈ÅADNIE nastƒôpujƒÖce sekcje, u≈ºywajƒÖc nag≈Ç√≥wk√≥w `### numer. Nazwa sekcji` i **≈ºadnych innych nag≈Ç√≥wk√≥w H3 w tytu≈Çach sekcji raportu**:

### 1. Kluczowe Punkty Wsp√≥lne
(Wypunktuj tematy, podtematy, kluczowe informacje, perspektywy i style narracji, kt√≥re powtarzajƒÖ siƒô w wiƒôkszo≈õci analizowanych tekst√≥w. Skup siƒô na tym, co jest standardem w TOP 10 i skonstruuj wytyczne dla copywritera)

### 2. Unikalne i Wyr√≥≈ºniajƒÖce Siƒô Elementy
(Wypunktuj nietypowe, oryginalne, innowacyjne lub szczeg√≥lnie warto≈õciowe informacje, dane, przyk≈Çady, case studies, infografiki (opisz co przedstawiajƒÖ) lub perspektywy, kt√≥re pojawi≈Çy siƒô tylko w niekt√≥rych ≈∫r√≥d≈Çach i mogƒÖ stanowiƒá przewagƒô konkurencyjnƒÖ dla nowego artyku≈Çu.)

### 3. Sugerowane S≈Çowa Kluczowe i Semantyka
(Na podstawie analizy tre≈õci konkurencji, stw√≥rz listƒô 10-12 najwa≈ºniejszych s≈Ç√≥w kluczowych, fraz d≈Çugoogonowych i pojƒôƒá semantycznie powiƒÖzanych. Pogrupuj je tematycznie, je≈õli to u≈Çatwia zrozumienie. Wska≈º intencjƒô wyszukiwania dla frazy g≈Ç√≥wnej.)

### 4. Proponowana Struktura Artyku≈Çu (Szkic)
(Zaproponuj idealnƒÖ, rozbudowanƒÖ strukturƒô nowego artyku≈Çu. Zacznij od propozycji chwytliwego tytu≈Çu (jako nag≈Ç√≥wek H1, np. `# Tytu≈Ç`). Nastƒôpnie stw√≥rz kompletnƒÖ listƒô nag≈Ç√≥wk√≥w dla artyku≈Çu, zawierajƒÖcƒÖ **co najmniej 4-5 g≈Ç√≥wnych sekcji (nag≈Ç√≥wki H2, np. `## Nag≈Ç√≥wek H2`)**. Dla ka≈ºdej g≈Ç√≥wnej sekcji H2, tam gdzie to merytorycznie uzasadnione, zaproponuj 2-3 podpunkty (nag≈Ç√≥wki H3, np. `### Nag≈Ç√≥wek H3`). Ca≈Ça struktura powinna byƒá logiczna, kompleksowo pokrywaƒá temat i wykorzystywaƒá wnioski z poprzednich sekcji analizy.)

### 5. Sekcja FAQ (Pytania i Odpowiedzi)
(Stw√≥rz listƒô 4-5 najczƒôstszych pyta≈Ñ, na kt√≥re odpowiadajƒÖ konkurenci, w stylu 'People Also Ask'. Podaj 2-3 zdaniowe bezpo≈õrednie odpowiedzi na te pytania, bazujƒÖc na analizowanej tre≈õci. Odpowiedzi napisz pod pytaniami)


Pamiƒôtaj, aby Twoja odpowied≈∫ by≈Ça TYLKO tre≈õciƒÖ raportu w formacie Markdown, bez ≈ºadnych dodatkowych wstƒôp√≥w czy podsumowa≈Ñ poza strukturƒÖ raportu. Ca≈Ça odpowied≈∫ musi byƒá w jƒôzyku polskim.
Tre≈õƒá do analizy:
{all_content}
"""
    # === KONIEC ZMODYFIKOWANEGO PROMPTU ===

    try:
        # U≈ºywamy modelu gemini-1.5-flash-latest dla szybko≈õci i koszt√≥w
        model = genai.GenerativeModel('gemini-1.5-flash-latest')
        response = model.generate_content(prompt)

        # Sprawdzenie, czy odpowied≈∫ zawiera tre≈õƒá
        if hasattr(response, 'text') and response.text:
             return response.text
        else:
             st.warning("‚ö†Ô∏è Gemini zwr√≥ci≈Ço pustƒÖ odpowied≈∫ lub b≈ÇƒÖd. Spr√≥buj ponownie lub zmie≈Ñ prompt.")
             # Dodatkowe informacje o b≈Çƒôdzie z API Gemini
             if hasattr(response, 'prompt_feedback'):
                 st.write("Feedback z promptu:", response.prompt_feedback)
             if hasattr(response, 'candidates') and response.candidates:
                  if response.candidates[0].finish_reason:
                    st.write("Przyczyna zako≈Ñczenia generacji przez API:", response.candidates[0].finish_reason)
                  if hasattr(response.candidates[0], 'safety_ratings'):
                     st.write("Oceny bezpiecze≈Ñstwa:", response.candidates[0].safety_ratings)

             return None


    except Exception as e:
        st.error(f"üõë B≈ÇƒÖd podczas komunikacji z Gemini API: {e}")
        # st.info("Upewnij siƒô, ≈ºe Tw√≥j GEMINI_API_KEY jest poprawny i masz dostƒôp do modelu 'gemini-1.5-flash-latest'.") # Wskaz√≥wka
        return None


# --- Funkcja do parsowania raportu (bez zmian) ---
def parse_report(report_text):
    """Dzieli pe≈Çny raport na sekcje do wy≈õwietlenia w zak≈Çadkach."""
    if not report_text: return {}
    sections = {}
    # Wyra≈ºenie regularne do znalezienia tre≈õci pomiƒôdzy nag≈Ç√≥wkami ###
    pattern = r"###\s*(?:\d+\.\s*)?(.*?)\n(.*?)(?=\n###\s*|$|\Z)"

    matches = re.findall(pattern, report_text, re.DOTALL)

    for match in matches:
        title = match[0].strip()
        content = match[1].strip()
        if title:
            sections[title] = content

    return sections


# ==============================================================================
# Krok 5: Interfejs U≈ºytkownika Streamlit i g≈Ç√≥wna logika
# ==============================================================================

# Pole formularza do wprowadzenia frazy
keyword = st.text_input("Wprowad≈∫ frazƒô kluczowƒÖ, kt√≥rƒÖ chcesz przeanalizowaƒá:", placeholder="np. jak dbaƒá o buty sk√≥rzane")

# Przycisk do uruchomienia analizy
if st.button("üöÄ Wygeneruj Kompleksowy Audyt SEO"):
    if not keyword:
        st.warning("Proszƒô wpisaƒá frazƒô kluczowƒÖ.")
        st.stop()

    if 'GEMINI_API_KEY' not in st.secrets or 'SEARCH_API_KEY' not in st.secrets or 'SEARCH_ENGINE_ID' not in st.secrets or 'SCRAPINGBEE_API_KEY' not in st.secrets:
         st.error("B≈ÇƒÖd: Nie wszystkie klucze API sƒÖ skonfigurowane w Streamlit Secrets.")
         st.stop()


    with st.spinner("Przeprowadzam pe≈Çny audyt... To mo≈ºe potrwaƒá kilka minut."):

        # Etap 1: Pobieranie wynik√≥w z Google
        st.info("Etap 1/4: Pobieranie i filtrowanie wynik√≥w z Google...")
        top_results = get_top_10_results(SEARCH_API_KEY, SEARCH_ENGINE_ID, keyword)

        if not top_results:
            st.error(f"Nie znaleziono ≈ºadnych wynik√≥w TOP 10 dla frazy: '{keyword}'.")
            st.stop()

        BANNED_DOMAINS = [
            "youtube.com", "pinterest.", "instagram.com", "facebook.com",
            "olx.pl", "allegro.pl", "twitter.com", "tiktok.com",
            "wikipedia.org", "s≈Çownik.pl", "encyklopedia.", "forum.",
            ".gov", ".edu",
            "otodom.pl", "gratka.pl", "domiporta.pl"
        ]
        filtered_results = [r for r in top_results if r and r.get('link') and not any(b in r['link'].lower() for b in BANNED_DOMAINS)]

        if not filtered_results:
            st.error("Po filtracji nie pozosta≈Çy ≈ºadne artyku≈Çy do analizy (usuniƒôto strony wideo, social media, sklepy, fora, Wikipedia, og≈Çoszenia, itp.).")
            st.stop()

        if len(top_results) > len(filtered_results):
             st.info(f"Pominiƒôto {len(top_results) - len(filtered_results)} wynik√≥w, analizujƒô {len(filtered_results)} znalezionych artyku≈Ç√≥w.")

        st.subheader("Analizowane adresy URL (po filtracji):")
        for i, result in enumerate(filtered_results, 1):
            display_title = result.get('title', result.get('link', f"Brak tytu≈Çu dla {result.get('link', 'nieznany URL')}"))
            st.write(f"{i}. [{display_title}]({result.get('link', '#')})")


        # Etap 2: Scraping tre≈õci
        st.info("Etap 2/4: Pobieranie tre≈õci ze stron przez Scraping API...")
        all_articles_content, successful_sources = [], []
        progress_bar = st.progress(0)
        for i, result in enumerate(filtered_results):
             url = result.get('link')
             if url:
                 content = scrape_and_clean_content(url, SCRAPINGBEE_API_KEY)
                 if content:
                     all_articles_content.append(content)
                     successful_sources.append({'title': result.get('title', 'Brak tytu≈Çu'), 'link': url})
                 progress_bar.progress((i + 1) / len(filtered_results))
        progress_bar.empty()

        if not all_articles_content:
            st.error("Nie uda≈Ço siƒô pobraƒá tre≈õci z ≈ºadnej ze stron przy u≈ºyciu ScrapingBee. Sprawd≈∫ klucz API ScrapingBee, limity lub dostƒôpno≈õƒá stron. Czasami problemem sƒÖ te≈º bardzo rozbudowane strony.")
            st.stop()

        st.success(f"‚úÖ Pomy≈õlnie pobrano tre≈õci z {len(all_articles_content)} stron.")


        # <<< NOWY KOD: Obliczanie i wy≈õwietlanie ≈õredniej d≈Çugo≈õci tekstu >>>
        average_word_count = 0
        if all_articles_content:
            total_words = sum(len(text.split()) for text in all_articles_content)
            average_word_count = total_words // len(all_articles_content) # U≈ºywamy dzielenia ca≈Çkowitego dla czystej liczby


        # Etap 3: Analiza AI
        st.info("Etap 3/4: Generowanie kompleksowego raportu przez AI...")
        aggregated_content = "\n\n---\n\n".join(all_articles_content)
        full_report = analyze_content_with_gemini(aggregated_content, keyword)

        if not full_report:
             st.error("Generowanie raportu przez Gemini nie powiod≈Ço siƒô. Sprawd≈∫ logi lub spr√≥buj z innƒÖ frazƒÖ/kluczami API.")
             st.stop()


        # Etap 4: Formatowanie wynik√≥w
        st.info("Etap 4/4: Formatowanie wynik√≥w...")
        report_sections = parse_report(full_report)

        sources_text = "\n".join([f"- [{source['title']}]({source['link']})" for source in successful_sources])
        report_sections["Analizowane ≈πr√≥d≈Ça"] = "Poni≈ºej lista adres√≥w URL, kt√≥rych tre≈õƒá zosta≈Ça pomy≈õlnie pobrana i przeanalizowana przez AI:\n" + sources_text

        st.balloons()
        st.success("‚úÖ Audyt SEO gotowy!")

        st.markdown(f"--- \n## Audyt SEO i plan tre≈õci dla frazy: '{keyword}'")

        # <<< NOWY KOD: Wy≈õwietlanie metryki ze ≈õredniƒÖ d≈Çugo≈õciƒÖ >>>
        if average_word_count > 0:
            st.metric(
                label="≈örednia d≈Çugo≈õƒá analizowanych artyku≈Ç√≥w",
                value=f"~ {average_word_count} s≈Ç√≥w",
                help="Jest to przybli≈ºona ≈õrednia liczba s≈Ç√≥w w artyku≈Çach konkurencji. Mo≈ºe s≈Çu≈ºyƒá jako wskaz√≥wka co do oczekiwanej objƒôto≈õci nowego tekstu."
            )
            st.markdown("---") # Dodanie separatora dla lepszej czytelno≈õci

        # --- Interfejs z zak≈Çadkami (bez zmian) ---
        preferred_tab_order = [
            "Kluczowe Punkty Wsp√≥lne",
            "Unikalne i Wyr√≥≈ºniajƒÖce Siƒô Elementy",
            "Sugerowane S≈Çowa Kluczowe i Semantyka",
            "Proponowana Struktura Artyku≈Çu (Szkic)",
            "Sekcja FAQ (Pytania i Odpowiedzi)",
            "Wnioski i Rekomendacje",
            "Analizowane ≈πr√≥d≈Ça"
        ]

        actual_tab_titles = [
            title for title in preferred_tab_order if title in report_sections and report_sections[title].strip()
        ]

        if actual_tab_titles:
             sources_tab_title = "Analizowane ≈πr√≥d≈Ça"
             if sources_tab_title in actual_tab_titles:
                  actual_tab_titles.remove(sources_tab_title)

             final_tabs_list = actual_tab_titles + ([sources_tab_title] if sources_tab_title in report_sections and report_sections[sources_tab_title].strip() else [])
             tabs = st.tabs(final_tabs_list)

             tab_title_map = {i: title for i, title in enumerate(final_tabs_list)}

             for i in range(len(tabs)):
                 with tabs[i]:
                     current_title = tab_title_map[i]
                     st.header(current_title)
                     st.markdown(report_sections[current_title])
        else:
             st.warning("Brak danych do wy≈õwietlenia w zak≈Çadkach. Sprawd≈∫ odpowied≈∫ Gemini. Mo≈ºliwe, ≈ºe API nie zwr√≥ci≈Ço ≈ºadnej tre≈õci lub wszystkie sekcje sƒÖ puste.")

else:
    if keyword:
         st.info(f"Wprowadzono frazƒô: '{keyword}'. Kliknij przycisk powy≈ºej, aby rozpoczƒÖƒá analizƒô.")
