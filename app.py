# -*- coding: utf-8 -*-

# ==============================================================================
# Krok 0: Instalacja bibliotek
# ==============================================================================
# pip install streamlit requests trafilatura google-generativeai scrapingbee
# ==============================================================================
# Krok 1: Import bibliotek
# ==============================================================================
import streamlit as st
import requests
import re
from trafilatura import extract
import google.generativeai as genai
from urllib.parse import urlencode as encode_query_params
import json

# ==============================================================================
# Krok 2: Konfiguracja strony Streamlit
# ==============================================================================
st.set_page_config(page_title="SEO Content Powerhouse", page_icon="üöÄ", layout="wide")
st.title("üöÄ SEO Content Powerhouse z AI")
st.markdown("Narzƒôdzie do tworzenia kompletnych strategii contentowych na podstawie analizy TOP 10 wynik√≥w Google.")

# ==============================================================================
# Krok 3: Obs≈Çuga Kluczy API ze Streamlit Secrets
# ==============================================================================
try:
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
    SCRAPINGBEE_API_KEY = st.secrets["SCRAPINGBEE_API_KEY"]
    DATAFORSEO_LOGIN = st.secrets["DATAFORSEO_LOGIN"]
    DATAFORSEO_PASSWORD = st.secrets["DATAFORSEO_PASSWORD"]

    genai.configure(api_key=GEMINI_API_KEY)

except KeyError as e:
    missing_key = str(e).strip("'")
    st.error(f"üõë B≈ÇƒÖd konfiguracji sekret√≥w! Nie znaleziono wymaganego sekretu: {missing_key}. Upewnij siƒô, ≈ºe skonfigurowa≈Çe≈õ GEMINI_API_KEY, SCRAPINGBEE_API_KEY, DATAFORSEO_LOGIN i DATAFORSEO_PASSWORD.")
    st.stop()
except Exception as e:
    st.error(f"üõë WystƒÖpi≈Ç nieoczekiwany b≈ÇƒÖd podczas ≈Çadowania kluczy: {e}")
    st.stop()

# ==============================================================================
# Krok 4: Funkcje Backendowe
# ==============================================================================

@st.cache_data
def get_serp_data_with_dataforseo(login, password, query, num_results=10, location_code=2616, language_code='pl'):
    """
    Pobiera wyniki wyszukiwania Google (organiczne i AI Overview) u≈ºywajƒÖc API DataForSEO.
    Zwraca s≈Çownik: {'organic_results': [], 'ai_overview_text': "tekst lub None"}
    """
    # (bez zmian w tej funkcji - usuniƒôto tylko komentarze st.write)
    post_data = [{"keyword": query, "location_code": location_code, "language_code": language_code, "depth": num_results}]
    headers = {'Content-Type': 'application/json'}
    endpoint_url = "https://api.dataforseo.com/v3/serp/google/organic/live/regular"
    organic_results_list = []
    ai_overview_text_content = None
    try:
        response = requests.post(endpoint_url, auth=(login, password), headers=headers, json=post_data, timeout=60)
        response.raise_for_status()
        data = response.json()
        if data.get("status_code") == 20000 and data.get("tasks") and data["tasks"][0].get("result") and data["tasks"][0]["result"][0].get("items"):
            items = data["tasks"][0]["result"][0]["items"]
            for item in items:
                item_type = item.get("type")
                if item_type == "organic":
                    title = item.get("title")
                    link = item.get("url")
                    if title and link:
                        organic_results_list.append({'title': title, 'link': link})
                elif item_type == "ai_overview":
                    if item.get("text"): ai_overview_text_content = item.get("text")
                    elif item.get("description"): ai_overview_text_content = item.get("description")
                    elif item.get("paragraphs") and isinstance(item.get("paragraphs"), list):
                        ai_overview_text_content = "\n\n".join([p.get("text", "") for p in item.get("paragraphs") if p.get("text")])
        else:
            status_message = data.get("status_message", "Nieznany b≈ÇƒÖd.")
            tasks_error = ""
            if data.get("tasks") and data["tasks"][0].get("status_message") != "Ok.":
                tasks_error = f" B≈ÇƒÖd zadania: {data['tasks'][0]['status_code']} - {data['tasks'][0]['status_message']}"
            st.warning(f"DataForSEO API zwr√≥ci≈Ço nieoczekiwany status lub brak wynik√≥w: {status_message}{tasks_error}.")
        return {'organic_results': organic_results_list, 'ai_overview_text': ai_overview_text_content}
    except requests.exceptions.Timeout: st.error(f"üõë Przekroczono czas oczekiwania na DataForSEO dla '{query}'")
    except requests.exceptions.RequestException as e: st.error(f"üõë B≈ÇƒÖd komunikacji z DataForSEO: {e}")
    except Exception as e: st.error(f"üõë Nieoczekiwany b≈ÇƒÖd przetwarzania odpowiedzi z DataForSEO: {e}")
    return {'organic_results': [], 'ai_overview_text': None}

@st.cache_data
def scrape_and_clean_content(url_to_scrape, scrapingbee_api_key):
    """Pobiera i czy≈õci tre≈õƒá ze strony u≈ºywajƒÖc ScrapingBee."""
    # (bez zmian)
    try:
        response = requests.get(
            url='https://app.scrapingbee.com/api/v1/',
            params={'api_key': scrapingbee_api_key, 'url': url_to_scrape, 'premium_proxy': 'true', 'block_resources': 'false'},
            timeout=90
        )
        response.raise_for_status()
        extracted_text = extract(response.text, include_comments=False, include_tables=False, include_images=False)
        if not extracted_text: return None
        cleaned_text = re.sub(r'\s+', ' ', extracted_text).strip()
        return cleaned_text if len(cleaned_text) > 100 else None
    except: return None

@st.cache_data(show_spinner="AI analizuje tre≈õƒá...")
def analyze_content_with_gemini(all_content, keyword_phrase, ai_overview_text=None):
    """Analizuje zagregowanƒÖ tre≈õƒá, AI Overview i generuje raport z Gemini."""
    if not all_content and not ai_overview_text:
        return "Brak tre≈õci (artyku≈Ç√≥w i AI Overview) do analizy przez AI."

    ai_overview_instructions = ""
    if ai_overview_text:
        ai_overview_instructions = f"""
Przeanalizuj poni≈ºszy tekst AI Overview wygenerowany przez Google dla frazy "{keyword_phrase}":
---
{ai_overview_text}
---
Na podstawie tej analizy oraz Twojej wiedzy o SEO, sformu≈Çuj 5-7 konkretnych, praktycznych wskaz√≥wek dla tw√≥rc√≥w tre≈õci. Wskaz√≥wki powinny wyja≈õniaƒá, jakie elementy w ich w≈Çasnych tre≈õciach (np. bezpo≈õrednie odpowiedzi, struktura, u≈ºyte sformu≈Çowania, dane, przyk≈Çady) mog≈Çyby zwiƒôkszyƒá prawdopodobie≈Ñstwo, ≈ºe Google wykorzysta ich materia≈Çy do generowania podobnych AI Overviews. Skup siƒô na tym, co mo≈ºna zrobiƒá, aby tre≈õƒá by≈Ça "SGE-friendly".
"""
    else:
        ai_overview_instructions = f"""
Dla frazy "{keyword_phrase}" nie znaleziono AI Overview w dostarczonych danych. 
Mimo to, na podstawie analizy tre≈õci z artyku≈Ç√≥w TOP10 (dostarczonych w sekcji "Tre≈õƒá z artyku≈Ç√≥w TOP10 do analizy"), zidentyfikuj cechy tych tre≈õci, kt√≥re mog≈Çyby byƒá korzystne z punktu widzenia generowania AI Overviews (SGE) przez Google. Podaj 5-7 praktycznych wskaz√≥wek SEO, jak na podstawie tych najlepszych artyku≈Ç√≥w z TOP10 mo≈ºna tworzyƒá tre≈õci "SGE-friendly". Skup siƒô na jako≈õci, strukturze, bezpo≈õrednich odpowiedziach, E-E-A-T i byciu pomocnym dla u≈ºytkownika, czerpiƒÖc inspiracjƒô z analizowanych artyku≈Ç√≥w.
"""

    prompt = f"""
Jeste≈õ ≈õwiatowej klasy analitykiem SEO i strategiem content marketingu, specjalizujƒÖcym siƒô w tworzeniu wyczerpujƒÖcych i bardzo szczeg√≥≈Çowych konspekt√≥w artyku≈Ç√≥w. Twoim zadaniem jest przeanalizowanie dostarczonej tre≈õci z czo≈Çowych artyku≈Ç√≥w dla frazy "{keyword_phrase}" oraz potencjalnie tre≈õci AI Overview. Na tej podstawie wygeneruj kompleksowy raport w formacie Markdown.

Raport MUSI byƒá podzielony na DOK≈ÅADNIE nastƒôpujƒÖce sekcje, u≈ºywajƒÖc nag≈Ç√≥wk√≥w `### numer. Nazwa sekcji` i **≈ºadnych innych nag≈Ç√≥wk√≥w H3 w tytu≈Çach sekcji raportu**:

### 1. Kluczowe Punkty Wsp√≥lne
(Wypunktuj tematy, podtematy, kluczowe informacje, perspektywy i style narracji, kt√≥re powtarzajƒÖ siƒô w wiƒôkszo≈õci analizowanych tekst√≥w z TOP10. Skup siƒô na tym, co jest standardem i skonstruuj wytyczne dla copywritera)

### 2. Unikalne i Wyr√≥≈ºniajƒÖce Siƒô Elementy
(Wypunktuj nietypowe, oryginalne, innowacyjne lub szczeg√≥lnie warto≈õciowe informacje, dane, przyk≈Çady, case studies, infografiki (opisz co przedstawiajƒÖ) lub perspektywy, kt√≥re pojawi≈Çy siƒô tylko w niekt√≥rych ≈∫r√≥d≈Çach z TOP10 i mogƒÖ stanowiƒá przewagƒô konkurencyjnƒÖ dla nowego artyku≈Çu.)

### 3. Sugerowane S≈Çowa Kluczowe i Semantyka
(Na podstawie analizy tre≈õci konkurencji z TOP10, stw√≥rz listƒô 10-12 najwa≈ºniejszych s≈Ç√≥w kluczowych, fraz d≈Çugoogonowych i pojƒôƒá semantycznie powiƒÖzanych. Pogrupuj je tematycznie, je≈õli to u≈Çatwia zrozumienie. Wska≈º intencjƒô wyszukiwania dla frazy g≈Ç√≥wnej.)

### 4. Proponowana Struktura Artyku≈Çu (Szkic)
(Zaproponuj BARDZO ROZBUDOWANƒÑ i SZCZEG√ì≈ÅOWƒÑ strukturƒô nowego artyku≈Çu w formacie Markdown, bazujƒÖc na analizie TOP10. Struktura MUSI zawieraƒá **DOK≈ÅADNIE 5 (piƒôƒá) g≈Ç√≥wnych sekcji (nag≈Ç√≥wki H2)**. Dla **KA≈ªDEJ z tych piƒôciu g≈Ç√≥wnych sekcji (H2) zaproponuj DOK≈ÅADNIE 3 (trzy) bardziej szczeg√≥≈Çowe podpunkty (nag≈Ç√≥wki H3)**. Nag≈Ç√≥wki powinny byƒá anga≈ºujƒÖce i precyzyjnie opisywaƒá zawarto≈õƒá danego fragmentu. Dbaj o logiczny przep≈Çyw i kompleksowe pokrycie tematu. Uwzglƒôdnij kluczowe punkty, unikalne elementy i semantykƒô z analizy. Przyk≈Çadowy tytu≈Ç artyku≈Çu: [Zaproponuj 2-3 chwytliwe tytu≈Çy dla artyku≈Çu o frazie "{keyword_phrase}"])

### 5. Sekcja FAQ (Pytania i Odpowiedzi)
(Stw√≥rz listƒô 4-5 najczƒôstszych pyta≈Ñ, na kt√≥re odpowiadajƒÖ konkurenci z TOP10, w stylu 'People Also Ask'. Podaj 2-3 zdaniowe bezpo≈õrednie odpowiedzi na te pytania, bazujƒÖc na analizowanej tre≈õci. Odpowiedzi napisz pod pytaniami)

### 6. Wskaz√≥wki SEO dla AI Overviews (SGE)
{ai_overview_instructions}

Pamiƒôtaj, aby Twoja odpowied≈∫ by≈Ça TYLKO tre≈õciƒÖ raportu w formacie Markdown, bez ≈ºadnych dodatkowych wstƒôp√≥w czy podsumowa≈Ñ poza strukturƒÖ raportu. Ca≈Ça odpowied≈∫ musi byƒá w jƒôzyku polskim.
Tre≈õƒá z artyku≈Ç√≥w TOP10 do analizy (je≈õli dostƒôpna):
{all_content if all_content else "Brak tre≈õci z artyku≈Ç√≥w TOP10 do analizy."}
"""
    try:
        model = genai.GenerativeModel('gemini-1.5-flash-latest')
        generation_config = genai.types.GenerationConfig(max_output_tokens=8192) # Maksymalny dla flash
        response = model.generate_content(prompt, generation_config=generation_config)
        if hasattr(response, 'text') and response.text: return response.text
        else:
             st.warning("‚ö†Ô∏è Gemini zwr√≥ci≈Ço pustƒÖ odpowied≈∫ lub b≈ÇƒÖd.")
             if hasattr(response, 'prompt_feedback'): st.write("Feedback:", response.prompt_feedback)
             if hasattr(response, 'candidates') and response.candidates and response.candidates[0].finish_reason:
                 st.write("Przyczyna zako≈Ñczenia:", response.candidates[0].finish_reason)
             return None
    except Exception as e:
        st.error(f"üõë B≈ÇƒÖd komunikacji z Gemini API: {e}")
        return None

def parse_report(report_text):
    """Dzieli pe≈Çny raport na sekcje do wy≈õwietlenia w zak≈Çadkach."""
    # (bez zmian)
    if not report_text: return {}
    sections = {}
    pattern = r"###\s*(?:\d+\.\s*)?(.*?)\n(.*?)(?=\n###\s*|$|\Z)"
    matches = re.findall(pattern, report_text, re.DOTALL)
    for match in matches:
        title = match[0].strip()
        content = match[1].strip()
        if title: sections[title] = content
    return sections

# ==============================================================================
# Krok 5: Interfejs U≈ºytkownika i g≈Ç√≥wna logika
# ==============================================================================
# (bez zmian w tej sekcji)
keyword = st.text_input("Wprowad≈∫ frazƒô kluczowƒÖ, kt√≥rƒÖ chcesz przeanalizowaƒá:", placeholder="np. jak dbaƒá o buty sk√≥rzane")

if st.button("üöÄ Wygeneruj Kompleksowy Audyt SEO"):
    if not keyword:
        st.warning("Proszƒô wpisaƒá frazƒô kluczowƒÖ.")
        st.stop()

    if 'SCRAPINGBEE_API_KEY' not in st.secrets or \
       'GEMINI_API_KEY' not in st.secrets or \
       'DATAFORSEO_LOGIN' not in st.secrets or \
       'DATAFORSEO_PASSWORD' not in st.secrets:
         st.error("B≈ÇƒÖd: Nie wszystkie wymagane klucze API sƒÖ skonfigurowane w Streamlit Secrets.")
         st.stop()

    with st.spinner("Przeprowadzam pe≈Çny audyt... To mo≈ºe potrwaƒá kilka minut."):
        st.info("Etap 1/4: Pobieranie i filtrowanie wynik√≥w z Google (przez DataForSEO)...")
        serp_data = get_serp_data_with_dataforseo(DATAFORSEO_LOGIN, DATAFORSEO_PASSWORD, keyword)
        top_results, ai_overview_text_from_serp = [], None
        if serp_data:
            top_results = serp_data.get('organic_results', [])
            ai_overview_text_from_serp = serp_data.get('ai_overview_text')
        
        if not top_results and not ai_overview_text_from_serp:
            st.error("Nie uda≈Ço siƒô pobraƒá ≈ºadnych danych SERP. Audyt przerwany.")
            st.stop()
        if not top_results:
            st.warning(f"Nie znaleziono wynik√≥w organicznych dla '{keyword}'. Analiza bƒôdzie kontynuowana, je≈õli znaleziono AI Overview.")

        BANNED_DOMAINS = ["youtube.com", "pinterest.", "instagram.com", "facebook.com", "olx.pl", "allegro.pl", "twitter.com", "tiktok.com", "wikipedia.org", "s≈Çownik.pl", "encyklopedia.", "forum.", ".gov", ".edu", "otodom.pl", "gratka.pl", "domiporta.pl"]
        filtered_results = [r for r in top_results if r and r.get('link') and not any(b in r['link'].lower() for b in BANNED_DOMAINS)]

        if not filtered_results and not ai_overview_text_from_serp:
             st.error("Po filtracji brak artyku≈Ç√≥w i AI Overview do analizy.")
             st.stop()
        elif not filtered_results and ai_overview_text_from_serp:
             if ai_overview_text_from_serp: st.info("Brak artyku≈Ç√≥w po filtracji, ale jest AI Overview. Przechodzƒô do analizy AI.")
        elif filtered_results:
             if len(top_results) > len(filtered_results):
                  st.info(f"Pominiƒôto {len(top_results) - len(filtered_results)} wynik√≥w, analizujƒô {len(filtered_results)} artyku≈Ç√≥w.")
             st.subheader("Analizowane adresy URL (po filtracji):")
             for i, r in enumerate(filtered_results, 1): st.write(f"{i}. [{r.get('title', r.get('link'))}]({r.get('link', '#')})")

        all_articles_content_str, successful_sources_list = "", []
        if filtered_results:
            st.info("Etap 2/4: Pobieranie tre≈õci ze stron (ScrapingBee)...")
            all_articles_content_list = []
            progress_bar = st.progress(0)
            for i, result in enumerate(filtered_results):
                url = result.get('link')
                if url:
                    content = scrape_and_clean_content(url, SCRAPINGBEE_API_KEY)
                    if content:
                        all_articles_content_list.append(content)
                        # successful_sources_list.append({'title': result.get('title', 'Brak tytu≈Çu'), 'link': url}) # Usuniƒôto, bo zak≈Çadka ≈∫r√≥de≈Ç jest usuniƒôta
                    progress_bar.progress((i + 1) / len(filtered_results))
            progress_bar.empty()
            if not all_articles_content_list: st.warning("Nie uda≈Ço siƒô pobraƒá tre≈õci z ≈ºadnej ze stron.")
            else:
                st.success(f"‚úÖ Pomy≈õlnie pobrano tre≈õci z {len(all_articles_content_list)} stron.")
                all_articles_content_str = "\n\n---\n\n".join(all_articles_content_list)
        
        if not all_articles_content_str and not ai_overview_text_from_serp:
            st.error("Brak tre≈õci artyku≈Ç√≥w oraz brak AI Overview do analizy. Audyt przerwany.")
            st.stop()

        st.info("Etap 3/4: Generowanie raportu przez AI (Gemini)...")
        full_report = analyze_content_with_gemini(all_articles_content_str, keyword, ai_overview_text_from_serp)
        if not full_report:
             st.error("Generowanie raportu przez Gemini nie powiod≈Ço siƒô.")
             st.stop()

        st.info("Etap 4/4: Formatowanie wynik√≥w...")
        report_sections = parse_report(full_report)
        
        st.balloons()
        st.success("‚úÖ Audyt SEO gotowy!")
        st.markdown(f"--- \n## Audyt SEO i plan tre≈õci dla frazy: '{keyword}'")

        preferred_tab_order = [
            "Kluczowe Punkty Wsp√≥lne", "Unikalne i Wyr√≥≈ºniajƒÖce Siƒô Elementy",
            "Sugerowane S≈Çowa Kluczowe i Semantyka", "Proponowana Struktura Artyku≈Çu (Szkic)",
            "Sekcja FAQ (Pytania i Odpowiedzi)",
            "Wskaz√≥wki SEO dla AI Overviews (SGE)"
        ]
        
        actual_tab_titles = [title for title in preferred_tab_order if title in report_sections and report_sections[title].strip()]
        
        if actual_tab_titles:
            tabs = st.tabs(actual_tab_titles)
            tab_title_map = {i: title for i, title in enumerate(actual_tab_titles)}
            for i in range(len(tabs)):
                with tabs[i]:
                    current_title = tab_title_map[i]
                    st.header(current_title)
                    st.markdown(report_sections[current_title])
        else:
            st.warning("Brak danych do wy≈õwietlenia w zak≈Çadkach.")
else:
    if keyword: st.info(f"Wprowadzono frazƒô: '{keyword}'. Kliknij przycisk powy≈ºej, aby rozpoczƒÖƒá analizƒô.")
